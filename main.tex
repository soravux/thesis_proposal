\documentclass{report}
\title{Thesis proposal: Short-Term Outdoor Photometric Stereo}
\author{Yannick Hold-Geoffroy}
%\programme{Doctorat en g\'enie \'electrique}
%\annee{2015}


\date{\today}

%\usepackage{hyperref}
%\hypersetup{colorlinks,allcolors=ULlinkcolor}

%\frenchbsetup{%
%  CompactItemize=false,         % ne pas compacter les listes
%  ThinSpaceInFrenchNumbers=true % espace fine dans les nombres
%}


\usepackage[letterpaper, margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
%\bibliographystyle{unsrtnat}
\usepackage[numbers,sort&compress]{natbib}
%\usepackage[numbers]{natbib}
%\usepackage{cite}   % sort citation numbers automatically
\usepackage{notoccite}
\usepackage{url}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{gensymb}
\usepackage{xcolor}
%\usepackage{adjustbox}
%\usepackage{authblk}
% to control spacing in item lists
\usepackage{enumitem}
\usepackage[pagebackref=false,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{defs}

\linespread{1.5}


\begin{document}

%\frontmatter                    % pages liminaires

\maketitle
%\pagetitreonlyone                     % production des pages de titre

\tableofcontents

% Commands
\newcommand{\boldomega}{\boldsymbol \omega} % bold omega
\newcommand{\boldmu}{\boldsymbol \mu} % bold omega
\newcommand{\bolddelta}{\boldsymbol \delta} % bold delta

\newcommand\norm[1]{\left\lVert#1\right\rVert}

\newcommand\todo[1]{\textcolor{red}{#1}}

\graphicspath{{figures/}}


\chapter*{Symbols and notations}

\begin{table}[htbp]\caption{Symbols and notations}
\centering % to have the caption near the table
\begin{tabular}{r c p{10cm} }

\hline & & \\
$\langle \cdot, \cdot \rangle$      & $=$ & Scalar (dot) product \\
$\mathbf{x}$                        & $=$ & Vector \\
$X$                                 & $=$ & Matrix \\
$\omega$                            & $=$ & Angle \\
\hline
\end{tabular}
\label{tab:TableOfNotationForMyResearch}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}

Real world scanning of outdoors structures gained a lot of interest recently. The emergence of cheap and easy-to-use 3D sensors brought its share of new applications, enabling the digitization of the world by the masses. Most of common everyday 3D scanning devices are captive of the indoors, though. There is yet to find a way to give everyone the chance to scan building-scale objects, bridging the gap between indoor and outdoor scanning.

This thesis proposes to do so simply by using cameras, and an existing reconstruction technique dubbed "Photometric Stereo" (PS). The idea is to point a camera towards a scene to be scanned, and to capture a time-lapse sequence over time. Variations in the illumination conditions can be used by PS to reconstruct the shape of the observed scene. PS has been known in computer vision for more than 35 years, and is still an active area of research.

\section{Photometric Stereo}
\label{sec:ps_ori}

The first definition of PS, made in 1979 by Woodham~\cite{Woodham1979}, made a lot of assumptions to simplify the problem to its essence, such as these ones:

\begin{itemize} \setlength\itemsep{-0.2em}
  \item The object surface reflectance must be
  \vspace{-0.65em}\begin{itemize} \setlength\itemsep{0.1em}
    \item Lambertian;
    \item constant;
  \end{itemize} \vspace{-0.4em}
  \item Lighting is known;
  \item Lighting is a distant point light source;
  \item Sensors are noiseless;
  \item All the images are aligned.
\end{itemize}

Following these assumptions, the Lambertian image formation model for a single pixel is defined as
\begin{equation}
b_t =  \rho \; \mathbf{l} \mathbf{n}_t \quad,
\end{equation}
where pixel $t$ will have an intensity $b_t$ when the corresponding surface patch normal $\mathbf{n}_t$ of albedo $\rho$ is lit by a point light source with an incident vector $\mathbf{l}$. For the sake of simplification, its albedo. Likewise, the incident lighting vector $\mathbf{l}$ is scaled by its intensity.

This means that the appearance of a pixel in an image, with the aforementioned assumptions, is dependent on 1) the normal and albedo of a visible surface patch, and 2) the incident angle and intensity of the light. Woodham realized that if a pixel appearance depends on the surface normal, it meant that this surface normal can be found from a known pixel appearance. This means that the shape of an object (through its surface normals) could be obtained by observing the pixels appearance. But there's a problem: a single pixel intensity cannot explain all three degrees of freedom of the normal to be reconstructed (scaled by its albedo), leading to an underconstrained problem.

To solve this issue, it is possible to take a sequence of images, all from the same viewpoint, but with different lighting conditions. The shading difference between the different lighting conditions can constrain the problem correctly. In the case of a sequence of images, we define $L$ as the stacked incident vectors of all the $m$ images in the sequence (labeled $\mathbf{l}_{1}, \mathbf{l}_{2}, \dots \mathbf{l}_{m}$):
\begin{equation}
L =
\begin{bmatrix}
    \mathbf{l}_{1} \\
    \mathbf{l}_{2} \\
    \vdots \\
    \mathbf{l}_{m}
\end{bmatrix}
\quad,
\end{equation}
which can be used to define the appearance of all pixels over all the images of the sequence, as such:
\begin{equation}
\label{eq:lamb_refl}
\mathbf{b} =  \rho \mathbf{L} \mathbf{n} \quad.
\end{equation}

Solving eq.~\eqref{eq:lamb_refl} for $\mathbf{n}$ gives the relation
\begin{equation}
\label{eq:original_form}
\rho \mathbf{n} =  \mathbf{L}^{-1} \mathbf{b} \quad,
\end{equation}
giving birth to the Photometric Stereo technique.

$\mathbf{n}$ provides the structure of the scene visible in the image sequence, giving a single normal for each pixel of the image. This output is called a normal map, because it maps a surface normal to each surface patch visible by a pixel. Integrating this normal map results in an height map (also called depth map), which represents the height of the surface at each sample point. To summarize, PS outputs a normal map, the gradient of the height map.

A concrete example of PS can be seen in fig.~\ref{fig:PS_example}, where a sequence of images and its lighting directions are set as inputs of eq.~\eqref{eq:original_form} to give a normal map in output and its integrated surface.

The great strength of this technique is its output density: 3D information will be generated for every pixel of one input image. This means that a sequence of 5 megapixels images, as can be found on many current off-the-shelf cameras and cellphones, will generate an output of 5 million 3D points using PS. This quantity of information is impressive given the cost of point and shoot cameras and the ubiquity of cellphones.

Even with all the assumption we made, one issue remains: the lighting directions over the sequence must not be coplanar. If they are coplanar, eq.~\ref{eq:original_form} will result in an underconstrained system, leaving an unknown degree of freedom. This led Woodham to believe that PS ``does not apply to outdoor images taken at different times during the same day [...] since the sun's path across the sky is planar''.

%\begin{equation}
%\mathbf{b} = \rho L(\mathbf{\boldomega}) \langle \boldomega, {\bf n} \rangle \,,
%\end{equation}

\begin{figure}
\begin{tabular}{cccccc|ccc}
\includegraphics[width=.08\linewidth]{PS/cat_0.png} &
\includegraphics[width=.08\linewidth]{PS/cat_3.png} &
\includegraphics[width=.08\linewidth]{PS/cat_4.png} &
\includegraphics[width=.08\linewidth]{PS/cat_5.png} &
\includegraphics[width=.08\linewidth]{PS/cat_10.png} &
\includegraphics[width=.08\linewidth]{PS/cat_11.png} &
\includegraphics[width=.08\linewidth]{PS/cat_normal_map.png} &
\includegraphics[width=.04\linewidth]{PS/sphere_nm.png} &
\includegraphics[width=.18\linewidth]{PS/3d.png} \\
a) & b) & c) & d) & e) & f) & g) & h) & i)
\end{tabular}
\caption{a-f) Examples of inputs lit from different directions, g) normal map obtained from the original form of the Photometric Stereo algorithm, h) sphere normal map showed as example, i) reconstructed surface from 3 viewpoints.\newline
{\small input images from CSE 455, 2010 by Neel Joshi, Ira Kemelmacher and Ian Simon}
}
\label{fig:PS_example}
\end{figure}

\section{Outdoor Photometric Stereo}

A lot of work have been done on PS since its original definition. Contrary to what Woodham believed, PS has recently been applied to outdoor photographs, mainly images captures by outdoor cameras. Unfortunately, outdoor lighting is complex and uncontrollable, therefore the techniques proposed in this domain require capturing either months of data, or the illumination conditions at each frame, none of which is a very practical scenario for the casual user.

To make PS work outdoor, the traditionally employed approximation models such as directional lighting will be replaced by a new data-driven model of natural illumination learned from a large database of high quality sky photographs. This database will be captured over extended periods of time, and will contain thousands of photos of the sky in different illumination conditions. By observing the sky directly, the data-driven model will accurately capture what the likely natural illumination conditions are for a given scene. This enhanced lighting model will better constrain the PS optimization algorithm, allowing it to converge toward real or plausible sky illumination conditions. The resulting shape estimation will better explain the observed input images, giving increased result quality. The key to bring PS outdoor is to understand natural illumination.

\section{Thesis proposal}

The main goal of this project is to reconstruct the shape of large-scale outdoor scenes from short time intervals, without the need for capturing the lighting conditions.

In this thesis, I propose to:
\begin{enumerate}
  \item bring a deeper understanding of the way PS can work outdoors;
  \item develop practical algorithms that allow precise shape recovery under unknown, uncontrolled outdoor lighting.
\end{enumerate}
These objectives can be attained by a better comprehension of natural illumination and its impact on shading. This knowledge can then be leveraged to improve existing reconstruction algorithms or make new ones that works with complex and uncontrolled lighting conditions.

This thesis will bring answers to questions like:
\begin{itemize}
  \item What is the minimum timelapse interval required to perform PS outside?
  \item What are the characteristics a PS practitioner needs to check in a sky to perform outside?
  \item How can we reconstruct an object with natural illumination using PS?
  \item What is the optimal input for PS when considering a 24h interval?
  \item Can we merge PS with other techniques to improve its performance outside?
\end{itemize}

\section{Anticipated impact}

3D scanning has become a part of everyday life, with sensors such as the Kinect now in everyone's home, effectively bringing 3D scanning capabilities to the masses. Taking the Kinect as example, a plethora of new use cases emerged since its original endeavor to revitalize the entertainment industry. For instance, the 3D printing market that grew significantly over the past few years, brought a new need for scans of household objects. Augmented reality is another avenue that requires a lot of models of real world objects. These are but a tiny fraction of the applications of real world scanning.

While the Kinect enabled myriads of applications in indoor environments, using such sensors outdoors is close to impossible. These kind of sensors are so-called active sensors, meaning that they interact with the scene to work. This usually means projecting light in the scene and sensing it back. But this method is problematic outside: the light patterns they emit are completely drowned by the sunlight. In outdoor settings, the only current possibility is to use very expensive scanners, out of reach for the casual user due to their complexity and operational cost. The idea is to bring back the availability and cheapness of sensors such as the Kinect to the outside world.

The level of understanding I propose in this thesis would allow the design and implementation a 3D shape acquisition system that relies only on off-the-shelf cameras, capable of bringing high quality digitization capabilities to anyone with a camera, thus enabling the collaborative reconstruction of large scale outdoor environments. This has the potential of impacting many fields, such as the digital preservation of cultural heritage before it gets damaged by wars, natural disasters, or the passage of time; the replication of real environments in virtual scenarios for the training of first responders or to improve urban planning; the creation of novel, realistic environments for use in video games or special effects in the movie industry; etc. By making these tasks easier, I believe this project will have significant impact on 3D shape acquisition as a whole.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{State of the Art Review}
\label{c:sota}

% Expliquer les am√©liorations de la PS au travers du temps:
% \begin{itemize}
% 	\item Unknown lighting
% 	\item Unknown BRDF
% 	\item Robustness
% 	\item Outdoor (algorithme et analyse)
% 	\begin{itemize}
% 		\item Yu
% 		\item Boxin shi
% 	\end{itemize}
% \end{itemize}

Since its inception, Photometric Stereo has received a lot of attention throughout the years. Researchers tried to alleviate the restrictive assumptions of the original method, such as Lambertian reflectance, noiseless sensors and known lighting. This chapter will first relate briefly the major improvements made on PS over the years, and then focus on the efforts made to bring it outside the laboratory.


\section{Photometric Stereo}

As previously stated, PS has been studied extensively for many decades. Researchers worked to make the method more general by removing, or at least alleviating, the assumptions initially made.

\subsection{Surface reflectance}
% BRDF
One thing they did is make PS work on other surfaces than perfectly Lambertian ones. At first, specular reflections \cite{Ikeuchi1981} were studied and incorporated to the PS framework. This also brought the idea of distributed light sources instead of point light sources to the field, an important idea discussed later on. Over the years, most of the reflectance assumptions were removed, allowing PS to work on surfaces yielding varying reflectance using either a parametric~\cite{hertzmann-pami-05,goldman-tpami-10} or a data-driven approach~\cite{alldrin-cvpr-08}.

\subsection{Shape from Shading}
% SfS
A new technique called shape-from-shading~\cite{Horn1989} was born from Photometric stereo. In this technique, a bunch of priors is assumed to infer the structure from a single image instead of a sequence of images. Two interesting elements from this work are worth noting for general PS use: 1) the shadow detection and handling, and 2) uniform illumination (an ambient light source) is taken into account. This technique was further developed to take into account outdoor photometric cues on cloudy days~\cite{Langer1994}. This work recognized that cloudy days could be approximated as diffuse light sources and treated them differently than point light sources, a key insight that will be discussed in details in \ref{iccp15}. Lately, a framework to infer local shape based from shading cues was proposed~\cite{Xiong2013}, yielding interesting intuitions transferable to a PS algorithm. Even though work still continues on outdoors shape-from-shading~\cite{oxholm-eccv-12,johnson-cvpr-11,barron-pami-15}, it is of limited interest in the scope of this thesis. This is due to the fact that work on this technique has mainly focused on finding tight constraints, strong priors or semantic segmentation, which are interesting topics, but far from photometric and shading cues.

\subsection{Reconstruction algorithm improvements}
% Fusion with MVS
After the Shape from Shading spinoff, PS was also used in conjunction with other shape reconstruction techniques to enhance their performance. The main idea is to ally the strength of PS (usually its output density) with the strength of another technique. As an example, merging a Multi-View Stereo algorithm with PS was done with great success~\cite{HernandezEsteban2008}.
[Should I talk more about this?]

% Shadows and robustness
More recently, work has been done to increase the stability of and robustness to shadows, highlights, image noise~\cite{BarskyPetrou-pami-2003,ikehata-cvpr-12,ikehata-cvpr-14}.

\subsection{Lighting}
% Light sources arbitrary motion & Bas-Relief Ambiguity
The impact of illumination on PS has also been extensively studied. At first, still assuming point light sources, the case of unknown light directions was solved by using singular value decomposition along with a set of priors~\cite{Hayakawa1994}. This allowed to approximate the images lighting conditions and the surface normals jointly. It is worth of note that the reconstruction is always up to a bas-relief ambiguity in the case of unknown light sources~\cite{Belhumeur1999}. This means that every reconstruction with unknown light sources are up to a scaling factor that is impossible to determine theoretically.

% Optimal Illumination control
All this work suppose that the controlled light spans ``enough'' the space, meaning that the experimenter should stop when he feels he has enough data to work with. This brought the question: ``is there an optimal placement for the lights to optimize the reconstruction performance of PS?'' Many researchers thought that the optimal light placement was a tradeoff between ideal incident illumination and shadow coverage. Mathematically, having orthogonal light sources is optimal for the reconstruction, but where is it optimal? It was found that the optimal light position is a slant angle of 54.74\degree from the camera at equal distance in circles around it~\cite{spence-iwtas-03,drbohlav-iccv-05}. [figure]

% diffuse light
Contrarily to laboratory conditions, real world lighting is not purely directional. There is always an ambient illumination, also called uniform illumination. This ambient illumination is mainly due to reflections on surfaces like walls and floors and can be far from negligible when a strong light source such as the sun (through a window, for instance) is present. The impact of this ambient illumination on PS was recently looked into~\cite{Angelopoulou2013}. They show surprising results revealing that strong directional light is the most important factor to obtain good reconstruction performance. Useful results can be obtained even when the ambient illumination is up to nine times the strength of the directional lighting, as long as this directional lighting in itself is strong. Weak directional lighting produces bad results, even in the absence of ambient illumination.

% Arbitrary light sources
Research on indoors illumination made a big leap when generic lighting conditions were estimated alongside traditional PS~\cite{basri-ijcv-2007}. This work considered the illumination as a complete sphere around the scene instead of a sum of discrete point light sources. The lighting conditions recovered are, however, limited to low-frequencies. While it can be quite enough for simple materials, it won't work for materials exhibiting specularities or yielding non-Lambertian reflectance.

%Covering the vast amount of work done on PS as a whole is beyond the scope of this thesis proposal. The rest of the document will focus more closely on work that have considered PS on outdoor conditions.


\section{Outdoor Photometric Stereo}

% webcams
To tackle the new challenge that posed outdoor PS, a natural first strategy has been to experiment with Lambertian reflectance and to model the sun as a point light source, to match a well-studied lab condition. Unfortunately, approaches based on this model have practical limitations caused by the movement of the sun in the sky for a given day. Depending on the latitude and time of year, its trajectory may lie too close to a plane, yielding an under-constrained, two-source PS problem~\cite{hernandez-pami-11}. Possible solutions include waiting for a day when the sun trajectory is non-planar~\cite{shen-pg-14}, or capturing several months of data~\cite{ackermann-cvpr-12,abrams-eccv-12} to ensure good conditioning.

% single day
Recently, Shen~{\em et al.}~\cite{shen-pg-14} showed that, contrary to common belief, the sun path in the sky actually does not always lie within a perfect plane. Thus, PS reconstruction can sometimes be computed in a single day even with a point light source model. The main downside of this approach is that planarity of the sun path (\ie, conditioning of PS reconstruction) depends on the latitude and the time of year. More specifically, reconstruction becomes unstable at high latitudes near the winter solstice, and worldwide near the equinoxes.

% richer lighting models
To compensate for limited sun motion, a promising approach is to use richer models of illumination that account for additional atmospheric factors in the sky. Typically, more elaborate models of illumination is done by employing (hemi-)spherical high dynamic range (HDR) environment maps~\cite{debevec-siggraph-98,reinhard-book-05} as input to outdoor PS. Encouraging results have been reported in~\cite{yu-iccp-13} for outdoor images taken within an interval of just eight hours (in a single day). On one hand, full environment maps can be captured and used with calibrated PS algorithms~\cite{yu-iccp-13,shi-3dv-14,hung-wacv-15}. On the other hand, it is also possible to estimate part of the environment map without explicitly capturing it, by synthesizing a hemispherical model of the sky using physically-based models~\cite{inose-tcva-13,jung-cvpr-15}.

% hold-geoffroy
%The work presented below extends our initial analysis in~\cite{holdgeoffroy-iccp-15}. Rather than presenting a new reconstruction algorithm, in~\cite{holdgeoffroy-iccp-15} we conducted an empirical analysis of the same sky database to identify which days provide more favorable atmospheric conditions for outdoor PS. However, no consideration was given to the shortest time interval of data capture needed to obtain accurate reconstructions; all results were reported on at least 6 hours (a ``full day'') of captured data. Here, instead of comparing days, we focus on analyzing different time intervals within each day. We then show that 6 hours is actually more than necessary, and detail the relationship between the appearance of the sky hemisphere and the quality of PS reconstruction.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Existing contributions}
\label{ch:existing}

The work done so far on understanding outdoor illumination for use in PS is presented in this chapter. It mainly consist of data capture and its analysis.

First, the database of sky data is presented with its capture details and statistics. Then, our work on modeling the sky and its impact on one-day PS is presented~\cite{holdgeoffroy-iccp-15}. Lastly, our work on the link between what happens in the sky and the reconstruction performance is presented~\cite{holdgeoffroy-3dv-15}. The latter received an award for ``Best Paper (Runner Up)'' at 3DV 2015.

\section{HDR database}
\label{sec:hdrdb}

% From ICCP
%As one might expect, the answer to the question above is intrinsically tied to the orientation of a particular surface patch, the associated hemisphere of lighting directions observed by the patch, and the variation in lighting intensity in that hemisphere over the course of a day. So far, this question has only been explored in laboratory conditions or with simple directional illumination, where optimal lighting configurations can be theoretically derived~\cite{drbohlav-iccv-05,klaudiny-prl-14,shen-pg-14}. No attempt has been made at answering this question with more realistic illumination models in an outdoor setup, where lighting cannot be controlled and atmospheric effects are difficult to predict.

To analyze the influence of outdoor lighting on photometric stereo, we rely on a rich dataset of high dynamic range images of the sky, captured under a wide variety of conditions. We use the environment map database of \cite{lalonde-3dv-14}, which contains HDR images of the sky captured using the approach described in \cite{stumpfel-afrigraph-04}. We augment the dataset of~\cite{lalonde-3dv-14} with an additional set of images captured using a similar setup, but at a different geographical location. In all, the dataset we used for our analysis totals 3,800 illumination conditions, captured over 23 different days. To ensure the data is properly aligned temporally, the HDR sky photos were captured during a continuous 6 hour time interval on each of these days, from 10:30 until 16:30. Fig.~\ref{fig:database} shows examples of the sky environment maps used in our analysis. Note that while the examples have been tone mapped for display, the actual sky images have extremely high dynamic range, and span the full 22 stops required to properly capture outdoor lighting~\cite{stumpfel-afrigraph-04}. In addition, all the images are converted to grayscale before the analysis is performed.

% From JF's 3DV
%We introduce a novel dataset of image collections, where each image is associated with its ground truth HDR lighting conditions. In all, our dataset contains 1,850 images of 22 different outdoor landmarks, captured under 350 different illumination conditions, see fig. 2. Each image has high dynamic range, is radiometrically and geometrically calibrated, and is aligned with its corresponding light probe. In this section, we describe how the dataset was captured, calibrated and aligned. The dataset, software, and many additional results are available on the project website [1].

%In this work, we assume that outdoor scenes are illuminated by light emitted solely from the sun and sky, and ignore local illumination effects such as light bouncing off the ground or nearby objects. As such, we capture the outdoor lighting conditions with wide angle, HDR photographs of the entire sky hemisphere. To do so, we follow the approach proposed by Stumpfel et al. [22]. We captured seven exposures of the sky ranging from 1/8000 to 1 second, using a Canon EOS 5D Mark III camera installed on a tripod, and fitted with a SIGMA EXDG 8mm fisheye lens. A 3.0 ND filter was installed behind the lens, necessary to accurately measure the sun intensity. The exposures were stored as 14-bit RAW images at the full resolution of the camera. The camera was controlled using a Raspberry Pi via a USB connection, and the setup was mounted on the roof of a tall building to capture the entire sky hemisphere. The seven exposures were captured every two minutes over a span of between three and ten hours on 25 different days spread over a period of six months from June to December 2013. A total of 3,380 different lighting conditions were captured. The fisheye lens was radiometrically calibrated following [22] (to account for chromaticity shifts caused by the ND filter), geometrically calibrated using [19], and the resulting light probes mapped to the angular environment map representation [17] for storage in floating-point EXR format. We merged the seven exposures using [4] to create one HDR sky probe per exposure set. Because the camera may have shifted from one capture day to another, we automatically align all sky probes to the world reference frame.

%This was done by detecting the sun in at least 3 images for a given day, and by computing the rotation matrix which best aligned the detected positions and the real sun coordinates (obtained with [16]). For days when the sun was never visible, the probes were manually aligned using other aligned light probes as examples, and by matching visible buildings close to the horizon. The second row of fig. 2 shows examples sky probes captured with our system. Note that while the examples have been tone mapped for display, the actual sky probes have extremely high dynamic range (see fig. 3).

% Stats
~~~

\begin{figure*}[!th]
    \centering
    \setlength{\tabcolsep}{0pt}
	\newcommand{\customwidth}{.08\linewidth}
    \begin{tabular}{@{}rcccccccccccc@{}}
                                                     &
    \begin{minipage}{\customwidth}\centering\scriptsize 11:00 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 11:30 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 12:00 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 12:30 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 13:00 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 13:30 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 14:00 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 14:30 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 15:00 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 15:30 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 16:00 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 16:30 \end{minipage}
    \\
    \begin{sideways}\begin{minipage}{\customwidth}\centering \scriptsize 08/24/2013 \\ light clouds \vspace{5pt} \end{minipage}\end{sideways} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_110040.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_113038.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_120033.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_123024.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_130014.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_133006.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_140002.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_142960.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_145957.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_152946.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_155938.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_162933.jpg}
    \\
    \begin{sideways}\begin{minipage}{\customwidth}\centering \scriptsize 11/06/2013 \\ mixed \vspace{5pt} \end{minipage}\end{sideways} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_110951.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_112948.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_115943.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_122939.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_125937.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_132936.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_135932.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_142922.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_145915.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_152913.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_155906.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_163057.jpg}

    \\
    \begin{sideways}\begin{minipage}{\customwidth}\centering \scriptsize 11/08/2014 \\ heavy clouds \vspace{5pt} \end{minipage}\end{sideways} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_110025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_113025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_120025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_123025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_130025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_133025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_140025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_143025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_150025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_153025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_160025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_163025.jpg}

    \\

    \end{tabular}
   	\caption[]{Examples from our dataset of HDR outdoor illumination conditions. In all, our dataset contains 3,800 different illumination conditions, captured from 10:30 until 16:30, during 23 days, spread over ten months and at two geographical locations. Each image is stored in the 32-bit floating point EXR format, and shown tone mapped here for display (with $\gamma = 1.6$). The companion video\footnotemark shows time-lapse sequences for these sky environment maps.}
	\label{fig:database}
\end{figure*}

\section{What Is a Good Day for Outdoor Photometric Stereo?}
\label{iccp15}

%\input{iccp15.tex}

\section{$x$-hour Outdoor Photometric Stereo}
\label{3dv15}

Best theoretical reconstruction performance.
%\input{3dv15.tex}



%\begin{equation}
%b_t = \frac{\rho}{\pi} \int_{\Omega_{\mathbf n}} L_t(\mathbf{\boldomega}) \langle \boldomega, {\mathbf n} \rangle d\omega \,,
%\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Proposed contributions}
% Metho!


% Clear that the community has an interest
From the overwhelming positive reviews and great comments we received for our previous work, it seems clear to me that the community is interested in outdoors PS. Reviewers found the work ``refreshing'', praising its ``concrete actionable predictions that can be of interest for practitioners'' and the pertinence of this line of work. Based on this feedback from the community, I look forward to build upon the existing contributions.

% Based on the work already done
Previous contributions mainly focused on the analysis of the complex illumination conditions of outdoor scenes. This analysis brought us a good deal of knowledge that will enable me to enhance existing algorithms.

But first, something is missing among the community to continue experiments with outdoor PS: a database of diverse objects publicly available. Just as the sky database we built, I propose to capture and share unsaturated HDR images of objects under various natural illumination conditions.

Then, using this database and the previous work we did, I will be able to modify existing PS algorithms to work for outdoor conditions. This work will be split in steps: first, I'll tackle the case when the illumination is known and gradually move toward the case where only some object images are required. This includes merging ideas from other 3D reconstruction techniques to simplify the problem or correct some issues that PS is not able to fix in the general case.

After having improved existing PS reconstruction algorithms using knowledge from the day lit sky, I'll focus on the other things that a practitioner could do to make PS work in problematic cases.


\section{Database of objects}

As explained previously (\ref{sec:hdrdb}), having realized that the research community was missing it, we've built a publicly available database of unsaturated HDR sky captures. This led me to realize that something else is missing: the same database for objects instead of skies. This database would allow researchers to analyze the shading of surfaces in conjunction with their illumination conditions. The goal is to obtain and share pictures of objects synchronized with the sky captures. These images will effectively give a direct link between the aspect of a sky and the appearance of objects lit by it. More importantly for me, it will also give the inverse: given a picture of an object, what sky lit it.

% Cite AMOS http://amos.cse.wustl.edu/
Similar databases already exist in the community (e.g., AMOS~\cite{jacobs-cvpr-2007}, Yahoo!'s databases~\cite{thomee-arxiv-15}), leading me to believe that this kind of data is wanted and appreciated by the researchers. Data available is usually very rich, yielding a lot of different content and objects with various reflectances. The issue of this data, in our case, is the lighting conditions. A full HDR capture of the sky synchronized with the images is not available. Furthermore, it is often difficult to estimate what the illumination looked like. Images may, or may not, have GPS coordinates, sometimes with the wrong information. Date and time in the EXIF data of the images are often wrong or skewed. Camera orientation (such as azimuth) is mostly never provided with these databases. It is then hard to guess the position of the sun in the sky.

% small & big
The idea of the database would be to gather images of various objects. At first, simple matte surfaces will be the focus, since it is simpler to work with. Objects with more complex reflectances will come later on. Two key characteristics are important for my analysis: small and big objects. Small objects are mandatory, because we can obtain easily a ground truth mesh from them using a handheld 3D scanner. Having a ground truth will greatly help the development of reconstruction algorithm and the measurement of its performance. Big objects are also important, because they are the main interest of using outdoor PS. To scan building-scale objects, handheld 3D scanners require quite a long time to operate, making them impracticable in this case. On the long term, reconstructing such big objects, like sculptures and buildings, is the goal I set to achieve. Having the data ready to perform the rest of the project is a logical step in the right direction that this database will achieve. 

% Build on existing infrastructure
A lot of effort is required to build and share a new dataset. This is why I plan on reusing the infrastructure we already have for the sky database. The capture system is already available, as well as the sharing interface. All that is left is to take the captures and move them to the existing database. The goal is to be as efficient as possible and put the minimum effort into the system \emph{per se} to focus on the objects to be captured and the rest of the project.

% Technical side
On the technical side, a Canon 5D mark III camera will be used in addition to the system already in place for the sky capture described in \ref{sec:hdrdb}. This additional camera will perform 5 burst captures of the object, in order to gather all the information needed to build an unsaturated HDR photo. A small chrome sphere is inserted in this camera's field of view to allow calibration with the sky camera. Hence, the precise rotation will be known between both camera. A system based on ROS developed by interns\footnote{Available publicly at \url{https://github.com/lvsn/CameraNetwork}} orchestrates the camera network and synchronizes the captures among cameras.


\section{Calibrated outdoor reconstruction}
\label{sec:calib}

% Up to now, only analysis. Make something with it.
Until now, the work done revolves around the analysis of natural skies. The next step is to use this new knowledge to build a reconstruction algorithm. At first, many simplifications will be done to the problem to be able to modify an existing algorithm and make it work outside. Here are described what I want to achieve first, followed by the insights that will make it work.

% Calibrated case.
In the beginning, I assume the whole sky is known to the algorithm as well as its calibration with respect to the object pictures. This means that the lighting matrix $L$ is known \emph{a priori}. Object surfaces will be assumed to be mostly Lambertian, but are not required to be constant. While complex reflectances could be found relatively easily from the sky capture, this step will be necessary to simplify the upcoming uncalibrated version of the algorithm, discussed in \ref{sec:uncalib}.

The images must all be aligned. Image alignment or registration is another field of study which is solved for the case useful for PS. This is discussed in~\cite{ackermann-cvpr-12}, as previously explained. I will suppose the images were already aligned using such technique in the algorithm I propose.

% Explain the DLT
Two important differences are made from the original PS algorithm as defined in \ref{sec:ps_ori} and the basis algorithm I plan to build my work upon. First, it is possible to remove the albedo $\rho$ from the equation to cancel the effect of the surface reflectance, making the algorithm robust to albedo variability. Secondly, instead of solving for the normals, it is possible to solve for the surface estimation directly. 

To remove the albedo $\rho$ from these equation, the trick is to divide an image by another one. Mathematically, this pixelwise division on, let's say image 1 by image 2, is represented as

\begin{equation}
\frac{b_{t,1}}{b_{t,2}} = \frac{\rho_t \mathbf{l}_1 \mathbf{n}_t}{\rho_t \mathbf{l}_2 \mathbf{n}_t} \quad,
\end{equation}
for pixel $t$. The albedo $\rho_t$ cancels out, giving the relation
\begin{equation}
\label{eq:ratio_images}
\left( b_{t,1} \mathbf{l}_2 - b_{t,2} \mathbf{l}_1 \right) \mathbf{n}_t = 0  \quad.
\end{equation}

This relation was used a lot before~\cite{yu-iccp-13,wu-pami-06}. The algorithm usually consist of taking a denominator image minimally affected by shadows and highlights and dividing all the other images of the sequence by this one. While this removes the constant albedo assumption, it leaves us with an image less in our sequence. Another possibility is to take all the possible permutations of images, translating in a better robustness. The problem with this approach is that it makes the problem grow quadratically (see annexe \ref{anx:permutations}, which quickly becomes problematic to solve quickly on computers having a finite amount of memory. Stochastically taking a predetermined amount can do the trick, but more a more sound selection is discussed and planned later on.
 
Solving for the surface height instead of the normals can be obtained by analyzing the relation between the two. Given pixel $t$ and its depth $z_t$, its homologue normal $\mathbf{n}_t$ can be expressed as
\begin{equation}
r_t = 
\begin{bmatrix}
\nabla z_{t,x} \\
\nabla z_{t,y} \\
-1
\end{bmatrix}
\end{equation}
\begin{equation}
\mathbf{n}_t =
\frac{r_t}{\norm{r_t}} \quad.
\end{equation}
This supposes that the surface is pointing toward $-1$, hence the camera is at $[0, 0, -1]$.

When merged with the albedo removal technique (equation \eqref{eq:ratio_images}) and letting down the normalization, we obtain
\begin{equation}
\label{eq:pre-dlt}
\left( b_{t,1} \mathbf{l}_2 - b_{t,2} \mathbf{l}_1 \right)
\begin{bmatrix}
\nabla z_{t,x} \\
\nabla z_{t,y} \\
-1
\end{bmatrix}
= 0  \quad.
\end{equation}

This new PS formulation is the base of my work. In order to make it easier to handle, a Direct Linear Tranform (DLT) method is applied to rewrite equation~\eqref{eq:pre-dlt}, into an set of homogeneous linear equations.

To do so, we must consider the left part of the equation, $\left( b_{t,1} \mathbf{l}_2 - b_{t,2} \mathbf{l}_1 \right)$, as being a vector of three scalars, let's say $\left[ i_t, j_t, k_t \right]$. When considering the division of an image $a$ by $a'$ as a whole instead of single pixels, we get the vectors $\mathbf{i}_{aa'}$, $\mathbf{j}{aa'}$ and $\mathbf{k}{aa'}$. Equation \eqref{eq:pre-dlt} can now be rewritten as
\begin{equation}
\left[ \mathrm{diag}(\mathbf{i}_{aa'}) \; \mathrm{diag}(\mathbf{j}_{aa'}) \; \mathrm{diag}(\mathbf{k}_{aa'})\right]
\begin{bmatrix}
\nabla \mathbf{z}_{x} \\
\nabla \mathbf{z}_{y} \\
-1
\end{bmatrix}
= 0 \quad,
\end{equation}
which can be rewritten as
\begin{equation}
\left[ \mathrm{diag}(\mathbf{i}_{aa'}) \; \mathrm{diag}(\mathbf{j}_{aa'}) \right]
\begin{bmatrix}
\nabla \mathbf{z}_{x} \\
\nabla \mathbf{z}_{y} \\
\end{bmatrix}
= \mathbf{k}_{aa'} \quad.
\end{equation}

\todo{Redo this}
It is possible to reformulate this equation as such:
\begin{equation}
\begin{bmatrix}
D_{ii} & D_{ij} \\
D_{ij} & D_{jj} \\
\end{bmatrix}
z =
\begin{bmatrix}
D_i \mathbf{k} \\
D_j \mathbf{k} \\
\end{bmatrix} \quad,
\end{equation}
where $D_i = \mathrm{diag}(\mathbf{i}_{aa'})$ and $ D_{ij} = \mathrm{diag}\left(\mathbf{i}_{aa'} \circ \mathbf{j}_{aa'}\right)$ ($\circ$ being the elementwise, or Hadamard, product). This gives the DLT formulation of the PS problem with the two explained enhancements.

% Normalization
\todo{Normalization}

% What's coming up next
Four avenues will be discussed on how I plan to make a PS algorithm work for outdoors. First, using the observations we made in our analysis brought us some ideas as how to improve the reconstruction performance. Two of such ideas are discussed. Then, interesting elements found in other algorithms and their inclusion in the proposed algorithm will be explained. Lastly, shadow detection is discussed, as pixels in shadows are difficult to handle.

\subsection{Regularization}

% One thing we noticed during our analysis is that not all pixels nor axis are equal.
% Figure, axis uncertainty
During our analysis, we noticed that not all pixel have the same uncertainty. Certain portions of the sphere were more prone to exhibit erratic reconstructions than others. Moreover, we found that a single pixel would often not have the same uncertainty with regard to its direction. This means that a normal recovered from a pixel could have a high accuracy on its azimuth and be very inaccurate on its elevation. As previously mentioned, this is due to the lighting condition variations over the image sequence.

We could leverage this information by performing regularization. Regularization is a type of prior we can put into the problem to favor some kind of solution over others. The kind of solution to be favored is chosen arbitrarily. In the case of object surfaces, a prior often made is the assumption of smoothness. This means that the optimization will be biased toward smooth solutions, i.e. not having abrupt curves. This is the prior that I will use for the rest of the section.

\todo{Add figure about reg.?}
Since we are on a 2D surface, two types of regularization exist: isotropic and anisotropic. Isotropic regularization means enforcing the same penalty for abrupt changes, regardless of the direction, whether the $x$ or the $y$ axis. Anisotropic regularization is the contrary: penalty won't be the same depending on axis the roughness, even for the same irregularity in the surface.

This idea was exploited in~\cite{hernandez-pami-11}, where they used first and second order anisotropic shape regularization. They worked on the case where the sequence of images is limited to three images, one of which is partially in shadow. This case led them to define the characteristic curve as the constraint induced by two lights, meaning a curve that has one degree of freedom. All the pixels lying on this curve are constrained in one direction (along the characteristic curve), but have no information on the other direction.
\todo{figure 3 of Hernandez}

In order to deal with this unknown direction, they choose to perform shape regularization. Without regularization, the results may be quite far from the ground truth~\ref{fig:her-2}. 
\todo{figure 2}

The regularization must be performed along directions that are perpendicular to the characteristic curves, let's call them $\mathbf{u}$. Hence, the regularization they use is of the form
\begin{equation}
\alpha \lvert \mathbf{u}^T \nabla z \rvert ^2
\end{equation}
for the first order, and 
\begin{equation}
\beta \lvert \mathbf{u}^T H(z)\nabla z \rvert ^2
\end{equation}
for the second order. Here, $z$ is, as before, the height, $\alpha$ and $\beta$ are the regularization weights and $H(z)$ is the Hessian matrix of $z$, its second-order derivatives. While the first order works to smooth the slopes, the second order enforces smooth curvature.


\subsection{Selection}
% Discrepancy between simulation and real world captures. Investigate it. We witness: selecting some images of the sequence may improve or degrade dramatically reconstruction performance.


\subsection{Algorithmic improvements}
% MRF, reference Jung's, tell our differences.

% To do that, we'll need to merge some ideas from other techniques because sometimes, some pixels cannot be recovered. Bas-Relief Amb. Silouhettes, Data-Driven priors.

\subsection{Shadow detection}
3DV15 article with cool SD

\section{Uncalibrated outdoor reconstruction}
\label{sec:uncalib}

% . reconstruction, don't need lighting condition is a plus

% Data-driven approach, what can the DB tell us on the image.

\subsection{Half-calibrated case}
% First, reconstruction of the ground only.


\subsection{Mostly uncalibrated}
% Then, estimation of the sky using the GPS coordinates and date & time. Then, without date and time.


\subsection{Fully uncalibrated}
% Lastly fully uncalibrated

\section{Looking farther than the sun}

% Up until now, we only considered the atmosphere during the day for reconstruction. The main problem, coplanar.

\subsection{Day \& Night}
During the night, the moon presents another plane.

% Figures


\subsection{Artificial}

% Diffusers

% City lights


\subsection{Other techniques}



%\section{Augmentation avec d'autres techniques}

% from ICCP
%While these techniques can lead to well-defined solutions, they are not always practical in many scenarios with strict temporal or geographical constraints. A second strategy has therefore been to combine PS with other techniques such as multi-view stereo~\cite{inose-tcva-13,shi-3dv-14}, or use reference objects as in \cite{johnson-cvpr-11} or example-based PS~\cite{hertzmann-pami-05,ackermann-3dv-14}. But can we accurately reconstruct surface geometry simply based on the photometric cue in an outdoor setting, without overly restrictive temporal and geographical constraints?

%D√©crire l'am√©lioration que pourrait apporter la PS utilis√©e conjointement avec du SFM et la st√©r√©o multivues standard.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Schedule}

The problems will be explored in increasing order of complexity according to the following schedule. First, the hardware required to gather the sky illumination database will be setup on a tall roof nearby, and the data acquisition process will be continued over a period of several months to build a rich dataset. Second, experiments with existing PS techniques to reconstruct shape with known illumination will be conducted. Leaning on these experiments, the algorithms will then be adapted to require knowledge only of the sun direction, obtained from the capture timestamps. Finally we will experiment with images where the lighting conditions are unknown.

\begin{itemize}
	\item 2016h: Selection and Calibrated PS algorithm
	\item 2016e: Capture, Blind PS algorithm
	\item 2016a: Blind PS algorithm, Day \& night
	\item 2017h: Fusion with SFM
	\item 2017e: Capture, Fusion with Multiview Stereo Techniques
	\item 2017a:
	\item 2018h: √âcrire et soutenir.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}\label{conclusion}

Recap.

\chapter*{Acknowledgments}

Mathieu Garon
Julien Becirovski

\chapter*{Annexes}

\section{Permutations of pair of images}
\label{anx:permutations}

From a set of $n$ images, the quantity of possible pairs of images is expressed as a special case of the permutation without repetition:
\begin{equation}
\frac{n!}{\left( n - 2 \right)!} \quad.
\end{equation}

By the definition of the factorial, this equation can be simplified to
\begin{equation}
\frac{n \times (n-1) \times (n-2) \times (n-3) \times \cdots \times 2}{(n-2) \times (n-3) \times \cdots \times 2} \quad.
\end{equation}

By simplifying, we find that the quantity is exactly
\begin{equation}
n^2 - n\quad,
\end{equation}
giving a quadratic expression (also called $\mathcal{O}(n^2)$ in Bachmann-Landau notation).

{\small
\bibliographystyle{acm}
\bibliography{main}
}

\end{document}
