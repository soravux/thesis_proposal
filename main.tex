\documentclass{report}
\title{Thesis proposal: Short-Term Outdoor Photometric Stereo}
\author{Yannick Hold-Geoffroy}
%\programme{Doctorat en g\'enie \'electrique}
%\annee{2015}


\date{\today}

%\usepackage{hyperref}
%\hypersetup{colorlinks,allcolors=ULlinkcolor}

%\frenchbsetup{%
%  CompactItemize=false,         % ne pas compacter les listes
%  ThinSpaceInFrenchNumbers=true % espace fine dans les nombres
%}


\usepackage[letterpaper, margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
%\bibliographystyle{unsrtnat}
\usepackage[numbers,sort&compress]{natbib}
%\usepackage[numbers]{natbib}
%\usepackage{cite}   % sort citation numbers automatically
\usepackage{notoccite}
\usepackage{url}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{gensymb}
\usepackage{xcolor}
%\usepackage{adjustbox}
%\usepackage{authblk}
% to control spacing in item lists
\usepackage{enumitem}
\usepackage[pagebackref=false,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{defs}

\linespread{1.5}


\begin{document}

%\frontmatter                    % pages liminaires

\maketitle
%\pagetitreonlyone                     % production des pages de titre

\tableofcontents

% Commands
\newcommand{\boldomega}{\boldsymbol \omega} % bold omega
\newcommand{\boldmu}{\boldsymbol \mu} % bold omega
\newcommand{\bolddelta}{\boldsymbol \delta} % bold delta

\newcommand\norm[1]{\left\lVert#1\right\rVert}

\newcommand\todo[1]{\textcolor{red}{#1}}

\graphicspath{{figures/}}


\chapter*{Symbols and notations}

\begin{table}[htbp]\caption{Symbols and notations}
\centering % to have the caption near the table
\begin{tabular}{r c p{10cm} }

\hline & & \\
$\langle \cdot, \cdot \rangle$      & $=$ & Scalar (dot) product \\
$\mathbf{x}$                        & $=$ & Vector \\
$X$                                 & $=$ & Matrix \\
$\omega$                            & $=$ & Angle \\
\hline
\end{tabular}
\label{tab:TableOfNotationForMyResearch}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}

Real world scanning of outdoors structures gained a lot of interest recently. The emergence of cheap and easy-to-use 3D sensors brought its share of new applications, enabling the digitization of the world by the masses. Most of common everyday 3D scanning devices are captive of the indoors, though. There is yet to find a way to give everyone the chance to scan building-scale objects, bridging the gap between indoor and outdoor scanning.

This thesis proposes to do so simply by using cameras, and an existing reconstruction technique dubbed "Photometric Stereo" (PS). The idea is to point a camera towards a scene to be scanned, and to capture a time-lapse sequence over time. Variations in the illumination conditions can be used by PS to reconstruct the shape of the observed scene. PS has been known in computer vision for more than 35 years, and is still an active area of research.

\section{Photometric Stereo}
\label{sec:ps_ori}

The first definition of PS, made in 1979 by Woodham~\cite{Woodham1979}, made a lot of assumptions to simplify the problem to its essence, such as these ones:

\begin{itemize} \setlength\itemsep{-0.2em}
  \item The object surface reflectance must be
  \vspace{-0.65em}\begin{itemize} \setlength\itemsep{0.1em}
    \item Lambertian;
    \item constant;
  \end{itemize} \vspace{-0.4em}
  \item Lighting is known;
  \item Lighting is a distant point light source;
  \item Sensors are noiseless;
  \item All the images are aligned.
\end{itemize}

Following these assumptions, the Lambertian image formation model for a single pixel is defined as
\begin{equation}
b_t =  \rho \; \mathbf{l} \mathbf{n}_t \quad,
\end{equation}
where pixel $t$ will have an intensity $b_t$ when the corresponding surface patch normal $\mathbf{n}_t$ of albedo $\rho$ is lit by a point light source with an incident vector $\mathbf{l}$. For the sake of simplification, its albedo. Likewise, the incident lighting vector $\mathbf{l}$ is scaled by its intensity.

This means that the appearance of a pixel in an image, with the aforementioned assumptions, is dependent on 1) the normal and albedo of a visible surface patch, and 2) the incident angle and intensity of the light. Woodham realized that if a pixel appearance depends on the surface normal, it meant that this surface normal can be found from a known pixel appearance. This means that the shape of an object (through its surface normals) could be obtained by observing the pixels appearance. But there's a problem: a single pixel intensity cannot explain all three degrees of freedom of the normal to be reconstructed (scaled by its albedo), leading to an underconstrained problem.

To solve this issue, it is possible to take a sequence of images, all from the same viewpoint, but with different lighting conditions. The shading difference between the different lighting conditions can constrain the problem correctly. In the case of a sequence of images, we define $L$ as the stacked incident vectors of all the $m$ images in the sequence (labeled $\mathbf{l}_{1}, \mathbf{l}_{2}, \dots \mathbf{l}_{m}$):
\begin{equation}
L =
\begin{bmatrix}
    \mathbf{l}_{1} \\
    \mathbf{l}_{2} \\
    \vdots \\
    \mathbf{l}_{m}
\end{bmatrix}
\quad,
\end{equation}
which can be used to define the appearance of all pixels over all the images of the sequence, as such:
\begin{equation}
\label{eq:lamb_refl}
\mathbf{b} =  \rho \mathbf{L} \mathbf{n} \quad.
\end{equation}

Solving eq.~\eqref{eq:lamb_refl} for $\mathbf{n}$ gives the relation
\begin{equation}
\label{eq:original_form}
\rho \mathbf{n} =  \mathbf{L}^{-1} \mathbf{b} \quad,
\end{equation}
giving birth to the Photometric Stereo technique.

$\mathbf{n}$ provides the structure of the scene visible in the image sequence, giving a single normal for each pixel of the image. This output is called a normal map, because it maps a surface normal to each surface patch visible by a pixel. Integrating this normal map results in an height map (also called depth map), which represents the height of the surface at each sample point. To summarize, PS outputs a normal map, the gradient of the height map.

A concrete example of PS can be seen in fig.~\ref{fig:PS_example}, where a sequence of images and its lighting directions are set as inputs of eq.~\eqref{eq:original_form} to give a normal map in output and its integrated surface.

The great strength of this technique is its output density: 3D information will be generated for every pixel of one input image. This means that a sequence of 5 megapixels images, as can be found on many current off-the-shelf cameras and cellphones, will generate an output of 5 million 3D points using PS. This quantity of information is impressive given the cost of point and shoot cameras and the ubiquity of cellphones.

Even with all the assumption we made, one issue remains: the lighting directions over the sequence must not be coplanar. If they are coplanar, eq.~\ref{eq:original_form} will result in an underconstrained system, leaving an unknown degree of freedom. This led Woodham to believe that PS ``does not apply to outdoor images taken at different times during the same day [...] since the sun's path across the sky is planar''.

%\begin{equation}
%\mathbf{b} = \rho L(\mathbf{\boldomega}) \langle \boldomega, {\bf n} \rangle \,,
%\end{equation}

\begin{figure}
\begin{tabular}{cccccc|ccc}
\includegraphics[width=.08\linewidth]{PS/cat_0.png} &
\includegraphics[width=.08\linewidth]{PS/cat_3.png} &
\includegraphics[width=.08\linewidth]{PS/cat_4.png} &
\includegraphics[width=.08\linewidth]{PS/cat_5.png} &
\includegraphics[width=.08\linewidth]{PS/cat_10.png} &
\includegraphics[width=.08\linewidth]{PS/cat_11.png} &
\includegraphics[width=.08\linewidth]{PS/cat_normal_map.png} &
\includegraphics[width=.04\linewidth]{PS/sphere_nm.png} &
\includegraphics[width=.18\linewidth]{PS/3d.png} \\
a) & b) & c) & d) & e) & f) & g) & h) & i)
\end{tabular}
\caption{a-f) Examples of inputs lit from different directions, g) normal map obtained from the original form of the Photometric Stereo algorithm, h) sphere normal map showed as example, i) reconstructed surface from 3 viewpoints.\newline
{\small input images from CSE 455, 2010 by Neel Joshi, Ira Kemelmacher and Ian Simon}
}
\label{fig:PS_example}
\end{figure}

\section{Outdoor Photometric Stereo}

A lot of work have been done on PS since its original definition. Contrary to what Woodham believed, PS has recently been applied to outdoor photographs, mainly images captures by outdoor cameras. Unfortunately, outdoor lighting is complex and uncontrollable, therefore the techniques proposed in this domain require capturing either months of data, or the illumination conditions at each frame, none of which is a very practical scenario for the casual user.

To make PS work outdoor, the traditionally employed approximation models such as directional lighting will be replaced by a new data-driven model of natural illumination learned from a large database of high quality sky photographs. This database will be captured over extended periods of time, and will contain thousands of photos of the sky in different illumination conditions. By observing the sky directly, the data-driven model will accurately capture what the likely natural illumination conditions are for a given scene. This enhanced lighting model will better constrain the PS optimization algorithm, allowing it to converge toward real or plausible sky illumination conditions. The resulting shape estimation will better explain the observed input images, giving increased result quality. The key to bring PS outdoor is to understand natural illumination.

\section{Thesis proposal}

The main goal of this project is to reconstruct the shape of large-scale outdoor scenes from short time intervals, without the need for capturing the lighting conditions.

In this thesis, I propose to:
\begin{enumerate}
  \item bring a deeper understanding of the way PS can work outdoors;
  \item develop practical algorithms that allow precise shape recovery under unknown, uncontrolled outdoor lighting.
\end{enumerate}
These objectives can be attained by a better comprehension of natural illumination and its impact on shading. This knowledge can then be leveraged to improve existing reconstruction algorithms or make new ones that works with complex and uncontrolled lighting conditions.

This thesis will bring answers to questions like:
\begin{itemize}
  \item What is the minimum timelapse interval required to perform PS outside?
  \item What are the characteristics a PS practitioner needs to check in a sky to perform outside?
  \item How can we reconstruct an object with natural illumination using PS?
  \item What is the optimal input for PS when considering a 24h interval?
  \item Can we merge PS with other techniques to improve its performance outside?
\end{itemize}

\section{Anticipated impact}

3D scanning has become a part of everyday life, with sensors such as the Kinect now in everyone's home, effectively bringing 3D scanning capabilities to the masses. Taking the Kinect as example, a plethora of new use cases emerged since its original endeavor to revitalize the entertainment industry. For instance, the 3D printing market that grew significantly over the past few years, brought a new need for scans of household objects. Augmented reality is another avenue that requires a lot of models of real world objects. These are but a tiny fraction of the applications of real world scanning.

While the Kinect enabled myriads of applications in indoor environments, using such sensors outdoors is close to impossible. These kind of sensors are so-called active sensors, meaning that they interact with the scene to work. This usually means projecting light in the scene and sensing it back. But this method is problematic outside: the light patterns they emit are completely drowned by the sunlight. In outdoor settings, the only current possibility is to use very expensive scanners, out of reach for the casual user due to their complexity and operational cost. The idea is to bring back the availability and cheapness of sensors such as the Kinect to the outside world.

The level of understanding I propose in this thesis would allow the design and implementation a 3D shape acquisition system that relies only on off-the-shelf cameras, capable of bringing high quality digitization capabilities to anyone with a camera, thus enabling the collaborative reconstruction of large scale outdoor environments. This has the potential of impacting many fields, such as the digital preservation of cultural heritage before it gets damaged by wars, natural disasters, or the passage of time; the replication of real environments in virtual scenarios for the training of first responders or to improve urban planning; the creation of novel, realistic environments for use in video games or special effects in the movie industry; etc. By making these tasks easier, I believe this project will have significant impact on 3D shape acquisition as a whole.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{State of the Art Review}
\label{c:sota}

% Expliquer les améliorations de la PS au travers du temps:
% \begin{itemize}
% 	\item Unknown lighting
% 	\item Unknown BRDF
% 	\item Robustness
% 	\item Outdoor (algorithme et analyse)
% 	\begin{itemize}
% 		\item Yu
% 		\item Boxin shi
% 	\end{itemize}
% \end{itemize}

Since its inception, Photometric Stereo has received a lot of attention throughout the years. Researchers tried to alleviate the restrictive assumptions of the original method, such as Lambertian reflectance, noiseless sensors and known lighting. This chapter will first relate briefly the major improvements made on PS over the years, and then focus on the efforts made to bring it outside the laboratory.


\section{Photometric Stereo}

As previously stated, PS has been studied extensively for many decades. Researchers worked to make the method more general by removing, or at least alleviating, the assumptions initially made.

\subsection{Surface reflectance}
% BRDF
One thing they did is make PS work on other surfaces than perfectly Lambertian ones. At first, specular reflections \cite{Ikeuchi1981} were studied and incorporated to the PS framework. This also brought the idea of distributed light sources instead of point light sources to the field, an important idea discussed later on. Over the years, most of the reflectance assumptions were removed, allowing PS to work on surfaces yielding varying reflectance using either a parametric~\cite{hertzmann-pami-05,goldman-tpami-10} or a data-driven approach~\cite{alldrin-cvpr-08}.

\subsection{Shape from Shading}
% SfS
A new technique called shape-from-shading~\cite{Horn1989} was born from Photometric stereo. In this technique, a bunch of priors is assumed to infer the structure from a single image instead of a sequence of images. Two interesting elements from this work are worth noting for general PS use: 1) the shadow detection and handling, and 2) uniform illumination (an ambient light source) is taken into account. This technique was further developed to take into account outdoor photometric cues on cloudy days~\cite{Langer1994}. This work recognized that cloudy days could be approximated as diffuse light sources and treated them differently than point light sources, a key insight that will be discussed in details in \ref{iccp15}. Lately, a framework to infer local shape based from shading cues was proposed~\cite{Xiong2013}, yielding interesting intuitions transferable to a PS algorithm. Even though work still continues on outdoors shape-from-shading~\cite{oxholm-eccv-12,johnson-cvpr-11,barron-pami-15}, it is of limited interest in the scope of this thesis. This is due to the fact that work on this technique has mainly focused on finding tight constraints, strong priors or semantic segmentation, which are interesting topics, but far from photometric and shading cues.

\subsection{Reconstruction algorithm improvements}
% Fusion with MVS
After the Shape from Shading spinoff, PS was also used in conjunction with other shape reconstruction techniques to enhance their performance. The main idea is to ally the strength of PS (usually its output density) with the strength of another technique. As an example, merging a Multi-View Stereo algorithm with PS was done with great success~\cite{HernandezEsteban2008}.
[Should I talk more about this?]

% Shadows and robustness
More recently, work has been done to increase the stability of and robustness to shadows, highlights, image noise~\cite{BarskyPetrou-pami-2003,ikehata-cvpr-12,ikehata-cvpr-14}.

\subsection{Lighting}
% Light sources arbitrary motion & Bas-Relief Ambiguity
The impact of illumination on PS has also been extensively studied. At first, still assuming point light sources, the case of unknown light directions was solved by using singular value decomposition along with a set of priors~\cite{Hayakawa1994}. This allowed to approximate the images lighting conditions and the surface normals jointly. It is worth of note that the reconstruction is always up to a bas-relief ambiguity in the case of unknown light sources~\cite{Belhumeur1999}. This means that every reconstruction with unknown light sources are up to a scaling factor that is impossible to determine theoretically.

% Optimal Illumination control
All this work suppose that the controlled light spans ``enough'' the space, meaning that the experimenter should stop when he feels he has enough data to work with. This brought the question: ``is there an optimal placement for the lights to optimize the reconstruction performance of PS?'' Many researchers thought that the optimal light placement was a tradeoff between ideal incident illumination and shadow coverage. Mathematically, having orthogonal light sources is optimal for the reconstruction, but where is it optimal? It was found that the optimal light position is a slant angle of 54.74\degree from the camera at equal distance in circles around it~\cite{spence-iwtas-03,drbohlav-iccv-05}. [figure]

% diffuse light
Contrarily to laboratory conditions, real world lighting is not purely directional. There is always an ambient illumination, also called uniform illumination. This ambient illumination is mainly due to reflections on surfaces like walls and floors and can be far from negligible when a strong light source such as the sun (through a window, for instance) is present. The impact of this ambient illumination on PS was recently looked into~\cite{Angelopoulou2013}. They show surprising results revealing that strong directional light is the most important factor to obtain good reconstruction performance. Useful results can be obtained even when the ambient illumination is up to nine times the strength of the directional lighting, as long as this directional lighting in itself is strong. Weak directional lighting produces bad results, even in the absence of ambient illumination.

% Arbitrary light sources
Research on indoors illumination made a big leap when generic lighting conditions were estimated alongside traditional PS~\cite{basri-ijcv-2007}. This work considered the illumination as a complete sphere around the scene instead of a sum of discrete point light sources. The lighting conditions recovered are, however, limited to low-frequencies. While it can be quite enough for simple materials, it won't work for materials exhibiting specularities or yielding non-Lambertian reflectance.

%Covering the vast amount of work done on PS as a whole is beyond the scope of this thesis proposal. The rest of the document will focus more closely on work that have considered PS on outdoor conditions.


\section{Outdoor Photometric Stereo}

% webcams
To tackle the new challenge that posed outdoor PS, a natural first strategy has been to experiment with Lambertian reflectance and to model the sun as a point light source, to match a well-studied lab condition. Unfortunately, approaches based on this model have practical limitations caused by the movement of the sun in the sky for a given day. Depending on the latitude and time of year, its trajectory may lie too close to a plane, yielding an under-constrained, two-source PS problem~\cite{hernandez-pami-11}. Possible solutions include waiting for a day when the sun trajectory is non-planar~\cite{shen-pg-14}, or capturing several months of data~\cite{ackermann-cvpr-12,abrams-eccv-12} to ensure good conditioning.

% single day
Recently, Shen~{\em et al.}~\cite{shen-pg-14} showed that, contrary to common belief, the sun path in the sky actually does not always lie within a perfect plane. Thus, PS reconstruction can sometimes be computed in a single day even with a point light source model. The main downside of this approach is that planarity of the sun path (\ie, conditioning of PS reconstruction) depends on the latitude and the time of year. More specifically, reconstruction becomes unstable at high latitudes near the winter solstice, and worldwide near the equinoxes.

% richer lighting models
To compensate for limited sun motion, a promising approach is to use richer models of illumination that account for additional atmospheric factors in the sky. Typically, more elaborate models of illumination is done by employing (hemi-)spherical high dynamic range (HDR) environment maps~\cite{debevec-siggraph-98,reinhard-book-05} as input to outdoor PS. Encouraging results have been reported in~\cite{yu-iccp-13} for outdoor images taken within an interval of just eight hours (in a single day). On one hand, full environment maps can be captured and used with calibrated PS algorithms~\cite{yu-iccp-13,shi-3dv-14,hung-wacv-15}. On the other hand, it is also possible to estimate part of the environment map without explicitly capturing it, by synthesizing a hemispherical model of the sky using physically-based models~\cite{inose-tcva-13,jung-cvpr-15}.

% hold-geoffroy
%The work presented below extends our initial analysis in~\cite{holdgeoffroy-iccp-15}. Rather than presenting a new reconstruction algorithm, in~\cite{holdgeoffroy-iccp-15} we conducted an empirical analysis of the same sky database to identify which days provide more favorable atmospheric conditions for outdoor PS. However, no consideration was given to the shortest time interval of data capture needed to obtain accurate reconstructions; all results were reported on at least 6 hours (a ``full day'') of captured data. Here, instead of comparing days, we focus on analyzing different time intervals within each day. We then show that 6 hours is actually more than necessary, and detail the relationship between the appearance of the sky hemisphere and the quality of PS reconstruction.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Existing contributions}
\label{ch:existing}

The work done so far on understanding outdoor illumination for use in PS is presented in this chapter. It mainly consist of data capture and its analysis.

First, the database of sky data is presented with its capture details and statistics. Then, our work on modeling the sky and its impact on one-day PS is presented~\cite{holdgeoffroy-iccp-15}. Lastly, our work on the link between what happens in the sky and the reconstruction performance is presented~\cite{holdgeoffroy-3dv-15}. The latter received an award for ``Best Paper (Runner Up)'' at 3DV 2015.

\section{HDR database}
\label{sec:hdrdb}

% From ICCP
%As one might expect, the answer to the question above is intrinsically tied to the orientation of a particular surface patch, the associated hemisphere of lighting directions observed by the patch, and the variation in lighting intensity in that hemisphere over the course of a day. So far, this question has only been explored in laboratory conditions or with simple directional illumination, where optimal lighting configurations can be theoretically derived~\cite{drbohlav-iccv-05,klaudiny-prl-14,shen-pg-14}. No attempt has been made at answering this question with more realistic illumination models in an outdoor setup, where lighting cannot be controlled and atmospheric effects are difficult to predict.

To analyze the influence of outdoor lighting on photometric stereo, we rely on a rich dataset of high dynamic range images of the sky, captured under a wide variety of conditions. We use the environment map database of \cite{lalonde-3dv-14}, which contains HDR images of the sky captured using the approach described in \cite{stumpfel-afrigraph-04}. We augment the dataset of~\cite{lalonde-3dv-14} with an additional set of images captured using a similar setup, but at a different geographical location. In all, the dataset we used for our analysis totals 3,800 illumination conditions, captured over 23 different days. To ensure the data is properly aligned temporally, the HDR sky photos were captured during a continuous 6 hour time interval on each of these days, from 10:30 until 16:30. Fig.~\ref{fig:database} shows examples of the sky environment maps used in our analysis. Note that while the examples have been tone mapped for display, the actual sky images have extremely high dynamic range, and span the full 22 stops required to properly capture outdoor lighting~\cite{stumpfel-afrigraph-04}. In addition, all the images are converted to grayscale before the analysis is performed.

% From JF's 3DV
%We introduce a novel dataset of image collections, where each image is associated with its ground truth HDR lighting conditions. In all, our dataset contains 1,850 images of 22 different outdoor landmarks, captured under 350 different illumination conditions, see fig. 2. Each image has high dynamic range, is radiometrically and geometrically calibrated, and is aligned with its corresponding light probe. In this section, we describe how the dataset was captured, calibrated and aligned. The dataset, software, and many additional results are available on the project website [1].

%In this work, we assume that outdoor scenes are illuminated by light emitted solely from the sun and sky, and ignore local illumination effects such as light bouncing off the ground or nearby objects. As such, we capture the outdoor lighting conditions with wide angle, HDR photographs of the entire sky hemisphere. To do so, we follow the approach proposed by Stumpfel et al. [22]. We captured seven exposures of the sky ranging from 1/8000 to 1 second, using a Canon EOS 5D Mark III camera installed on a tripod, and fitted with a SIGMA EXDG 8mm fisheye lens. A 3.0 ND filter was installed behind the lens, necessary to accurately measure the sun intensity. The exposures were stored as 14-bit RAW images at the full resolution of the camera. The camera was controlled using a Raspberry Pi via a USB connection, and the setup was mounted on the roof of a tall building to capture the entire sky hemisphere. The seven exposures were captured every two minutes over a span of between three and ten hours on 25 different days spread over a period of six months from June to December 2013. A total of 3,380 different lighting conditions were captured. The fisheye lens was radiometrically calibrated following [22] (to account for chromaticity shifts caused by the ND filter), geometrically calibrated using [19], and the resulting light probes mapped to the angular environment map representation [17] for storage in floating-point EXR format. We merged the seven exposures using [4] to create one HDR sky probe per exposure set. Because the camera may have shifted from one capture day to another, we automatically align all sky probes to the world reference frame.

%This was done by detecting the sun in at least 3 images for a given day, and by computing the rotation matrix which best aligned the detected positions and the real sun coordinates (obtained with [16]). For days when the sun was never visible, the probes were manually aligned using other aligned light probes as examples, and by matching visible buildings close to the horizon. The second row of fig. 2 shows examples sky probes captured with our system. Note that while the examples have been tone mapped for display, the actual sky probes have extremely high dynamic range (see fig. 3).

% Stats
~~~

\begin{figure*}[!th]
    \centering
    \setlength{\tabcolsep}{0pt}
	\newcommand{\customwidth}{.08\linewidth}
    \begin{tabular}{@{}rcccccccccccc@{}}
                                                     &
    \begin{minipage}{\customwidth}\centering\scriptsize 11:00 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 11:30 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 12:00 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 12:30 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 13:00 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 13:30 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 14:00 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 14:30 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 15:00 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 15:30 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 16:00 \end{minipage} &
    \begin{minipage}{\customwidth}\centering\scriptsize 16:30 \end{minipage}
    \\
    \begin{sideways}\begin{minipage}{\customwidth}\centering \scriptsize 08/24/2013 \\ light clouds \vspace{5pt} \end{minipage}\end{sideways} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_110040.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_113038.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_120033.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_123024.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_130014.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_133006.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_140002.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_142960.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_145957.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_152946.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_155938.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20130824_162933.jpg}
    \\
    \begin{sideways}\begin{minipage}{\customwidth}\centering \scriptsize 11/06/2013 \\ mixed \vspace{5pt} \end{minipage}\end{sideways} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_110951.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_112948.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_115943.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_122939.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_125937.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_132936.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_135932.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_142922.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_145915.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_152913.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_155906.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20131106_163057.jpg}

    \\
    \begin{sideways}\begin{minipage}{\customwidth}\centering \scriptsize 11/08/2014 \\ heavy clouds \vspace{5pt} \end{minipage}\end{sideways} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_110025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_113025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_120025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_123025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_130025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_133025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_140025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_143025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_150025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_153025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_160025.jpg} &
    \includegraphics[width=\customwidth]{./figures/database/20141108_163025.jpg}

    \\

    \end{tabular}
   	\caption[]{Examples from our dataset of HDR outdoor illumination conditions. In all, our dataset contains 3,800 different illumination conditions, captured from 10:30 until 16:30, during 23 days, spread over ten months and at two geographical locations. Each image is stored in the 32-bit floating point EXR format, and shown tone mapped here for display (with $\gamma = 1.6$). The companion video\footnotemark shows time-lapse sequences for these sky environment maps.}
	\label{fig:database}
\end{figure*}

\section{What Is a Good Day for Outdoor Photometric Stereo?}
\label{iccp15}

%\input{iccp15.tex}

\section{$x$-hour Outdoor Photometric Stereo}
\label{3dv15}

Best theoretical reconstruction performance.
%\input{3dv15.tex}



%\begin{equation}
%b_t = \frac{\rho}{\pi} \int_{\Omega_{\mathbf n}} L_t(\mathbf{\boldomega}) \langle \boldomega, {\mathbf n} \rangle d\omega \,,
%\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Proposed contributions}
% Metho!


% Clear that the community has an interest
From the overwhelming positive reviews and great comments we received for our previous work, it seems clear to me that the community is interested in outdoors PS. Reviewers found the work ``refreshing'', praising its ``concrete actionable predictions that can be of interest for practitioners'' and the pertinence of this line of work. Based on this feedback from the community, I look forward to build upon the existing contributions.

% Based on the work already done
Previous contributions mainly focused on the analysis of the complex illumination conditions of outdoor scenes. This analysis brought us a good deal of knowledge that will enable me to enhance existing algorithms.

But first, something is missing among the community to continue experiments with outdoor PS: a database of diverse objects publicly available. Just as the sky database we built, I propose to capture and share unsaturated HDR images of objects under various natural illumination conditions.

Then, using this database and the previous work we did, I will be able to modify existing PS algorithms to work for outdoor conditions. This work will be split in steps: first, I'll tackle the case when the illumination is known and gradually move toward the case where only some object images are required. This includes merging ideas from other 3D reconstruction techniques to simplify the problem or correct some issues that PS is not able to fix in the general case.

After having improved existing PS reconstruction algorithms using knowledge from the day lit sky, I'll focus on the other things that a practitioner could do to make PS work in problematic cases.


\section{Database of objects}

As explained previously (\ref{sec:hdrdb}), having realized that the research community was missing it, we've built a publicly available database of unsaturated HDR sky captures. This led me to realize that something else is missing: the same database for objects instead of skies. This database would allow researchers to analyze the shading of surfaces in conjunction with their illumination conditions. The goal is to obtain and share pictures of objects synchronized with the sky captures. These images will effectively give a direct link between the aspect of a sky and the appearance of objects lit by it. More importantly for me, it will also give the inverse: given a picture of an object, what sky lit it.

% Cite AMOS http://amos.cse.wustl.edu/
Similar databases already exist in the community (e.g., AMOS~\cite{jacobs-cvpr-2007}, Yahoo!'s databases~\cite{thomee-arxiv-15}), leading me to believe that this kind of data is wanted and appreciated by the researchers. Data available is usually very rich, yielding a lot of different content and objects with various reflectances. The issue of this data, in our case, is the lighting conditions. A full HDR capture of the sky synchronized with the images is not available. Furthermore, it is often difficult to estimate what the illumination looked like. Images may, or may not, have GPS coordinates, sometimes with the wrong information. Date and time in the EXIF data of the images are often wrong or skewed. Camera orientation (such as azimuth) is mostly never provided with these databases. It is then hard to guess the position of the sun in the sky.

% small & big
The idea of the database would be to gather images of various objects. At first, simple matte surfaces will be the focus, since it is simpler to work with. Objects with more complex reflectances will come later on. Two key characteristics are important for my analysis: small and big objects. Small objects are mandatory, because we can obtain easily a ground truth mesh from them using a handheld 3D scanner. Having a ground truth will greatly help the development of reconstruction algorithm and the measurement of its performance. Big objects are also important, because they are the main interest of using outdoor PS. To scan building-scale objects, handheld 3D scanners require quite a long time to operate, making them impracticable in this case. On the long term, reconstructing such big objects, like sculptures and buildings, is the goal I set to achieve. Having the data ready to perform the rest of the project is a logical step in the right direction that this database will achieve. 

% Build on existing infrastructure
A lot of effort is required to build and share a new dataset. This is why I plan on reusing the infrastructure we already have for the sky database. The capture system is already available, as well as the sharing interface. All that is left is to take the captures and move them to the existing database. The goal is to be as efficient as possible and put the minimum effort into the system \emph{per se} to focus on the objects to be captured and the rest of the project.

% Technical side
On the technical side, a Canon 5D mark III camera will be used in addition to the system already in place for the sky capture described in \ref{sec:hdrdb}. This additional camera will perform 5 burst captures of the object, in order to gather all the information needed to build an unsaturated HDR photo. A small chrome sphere is inserted in this camera's field of view to allow calibration with the sky camera. Hence, the precise rotation will be known between both camera. A system based on ROS developed by interns\footnote{Available publicly at \url{https://github.com/lvsn/CameraNetwork}} orchestrates the camera network and synchronizes the captures among cameras.


\section{Calibrated outdoor reconstruction}
\label{sec:calib}

% Up to now, only analysis. Make something with it.
Until now, the work done revolves around the analysis of natural skies. The next step is to use this new knowledge to build a reconstruction algorithm. At first, many simplifications will be done to the problem to be able to modify an existing algorithm and make it work outside. Here are described what I want to achieve first, followed by the insights that will make it work.

% Calibrated case.
In the beginning, I assume the whole sky is known to the algorithm as well as its calibration with respect to the object pictures. This means that the lighting matrix $L$ is known \emph{a priori}. Object surfaces will be assumed to be mostly Lambertian, but are not required to be constant. While complex reflectances could be found relatively easily from the sky capture, this step will be necessary to simplify the upcoming uncalibrated version of the algorithm, discussed in \ref{sec:uncalib}.

The images must all be aligned. Image alignment or registration is another field of study which is solved for the case useful for PS. This is discussed in~\cite{ackermann-cvpr-12}, as previously explained. I will suppose the images were already aligned using such technique in the algorithm I propose.

% Explain the DLT
Two important differences are made from the original PS algorithm as defined in \ref{sec:ps_ori} and the basis algorithm I plan to build my work upon. First, it is possible to remove the albedo $\rho$ from the equation to cancel the effect of the surface reflectance, making the algorithm robust to albedo variability. Secondly, instead of solving for the normals, it is possible to solve for the surface estimation directly. 

To remove the albedo $\rho$ from these equation, the trick is to divide an image by another one. Mathematically, this pixelwise division on, let's say image 1 by image 2, is represented as

\begin{equation}
\frac{b_{t,1}}{b_{t,2}} = \frac{\rho_t \mathbf{l}_1 \mathbf{n}_t}{\rho_t \mathbf{l}_2 \mathbf{n}_t} \quad,
\end{equation}
for pixel $t$. The albedo $\rho_t$ cancels out, giving the relation
\begin{equation}
\label{eq:ratio_images}
\left( b_{t,1} \mathbf{l}_2 - b_{t,2} \mathbf{l}_1 \right) \mathbf{n}_t = 0  \quad.
\end{equation}

This relation was used a lot before~\cite{yu-iccp-13,wu-pami-06}. The algorithm usually consist of taking a denominator image minimally affected by shadows and highlights and dividing all the other images of the sequence by this one. While this removes the constant albedo assumption, it leaves us with an image less in our sequence. Another possibility is to take all the possible permutations of images, translating in a better robustness. The problem with this approach is that it makes the problem grow quadratically (see annexe \ref{anx:permutations}, which quickly becomes problematic to solve quickly on computers having a finite amount of memory. Stochastically taking a predetermined amount can do the trick, but more a more sound selection is discussed and planned later on.
 
Solving for the surface height instead of the normals can be obtained by analyzing the relation between the two. Given pixel $t$ and its depth $z_t$, its homologue normal $\mathbf{n}_t$ can be expressed as
\begin{equation}
r_t = 
\begin{bmatrix}
\nabla z_{t,x} \\
\nabla z_{t,y} \\
-1
\end{bmatrix}
\end{equation}
\begin{equation}
\mathbf{n}_t =
\frac{r_t}{\norm{r_t}} \quad.
\end{equation}
This supposes that the surface is pointing toward $-1$, hence the camera is at $[0, 0, -1]$.

When merged with the albedo removal technique (equation \eqref{eq:ratio_images}) and letting down the normalization, we obtain
\begin{equation}
\label{eq:pre-dlt}
\left( b_{t,1} \mathbf{l}_2 - b_{t,2} \mathbf{l}_1 \right)
\begin{bmatrix}
\nabla z_{t,x} \\
\nabla z_{t,y} \\
-1
\end{bmatrix}
= 0  \quad.
\end{equation}

This new PS formulation is the base of my work. In order to make it easier to handle, a Direct Linear Tranform (DLT) method is applied to rewrite equation~\eqref{eq:pre-dlt}, into an set of homogeneous linear equations.

To do so, we must consider the left part of the equation, $\left( b_{t,1} \mathbf{l}_2 - b_{t,2} \mathbf{l}_1 \right)$, as being a vector of three scalars, let's say $\left[ i_t, j_t, k_t \right]$. When considering the division of an image $a$ by $a'$ as a whole instead of single pixels, we get the vectors $\mathbf{i}_{aa'}$, $\mathbf{j}{aa'}$ and $\mathbf{k}{aa'}$. Equation \eqref{eq:pre-dlt} can now be rewritten as
\begin{equation}
\left[ \mathrm{diag}(\mathbf{i}_{aa'}) \; \mathrm{diag}(\mathbf{j}_{aa'}) \; \mathrm{diag}(\mathbf{k}_{aa'})\right]
\begin{bmatrix}
\nabla \mathbf{z}_{x} \\
\nabla \mathbf{z}_{y} \\
-1
\end{bmatrix}
= 0 \quad,
\end{equation}
which can be rewritten as
\begin{equation}
\left[ \mathrm{diag}(\mathbf{i}_{aa'}) \; \mathrm{diag}(\mathbf{j}_{aa'}) \right]
\begin{bmatrix}
\nabla \mathbf{z}_{x} \\
\nabla \mathbf{z}_{y} \\
\end{bmatrix}
= \mathbf{k}_{aa'} \quad.
\end{equation}

\todo{Redo this}
It is possible to reformulate this equation as such:
\begin{equation}
\begin{bmatrix}
D_{ii} & D_{ij} \\
D_{ij} & D_{jj} \\
\end{bmatrix}
z =
\begin{bmatrix}
D_i \mathbf{k} \\
D_j \mathbf{k} \\
\end{bmatrix} \quad,
\end{equation}
where $D_i = \mathrm{diag}(\mathbf{i}_{aa'})$ and $ D_{ij} = \mathrm{diag}\left(\mathbf{i}_{aa'} \circ \mathbf{j}_{aa'}\right)$ ($\circ$ being the elementwise, or Hadamard, product). This gives the DLT formulation of the PS problem with the two explained enhancements.

% Normalization
\todo{Normalization}

% What's coming up next
Four avenues will be discussed on how I plan to make a PS algorithm work for outdoors. First, using the observations we made in our analysis brought us some ideas as how to improve the reconstruction performance. Two of such ideas are discussed. Then, interesting elements found in other algorithms and their inclusion in the proposed algorithm will be explained. Lastly, shadow detection is discussed, as pixels in shadows are difficult to handle.

\subsection{Regularization}

% One thing we noticed during our analysis is that not all pixels nor axis are equal.
% Figure, axis uncertainty
During our analysis, we noticed that not all pixel have the same uncertainty. Certain portions of the sphere were more prone to exhibit erratic reconstructions than others. Moreover, we found that a single pixel would often not have the same uncertainty with regard to its direction. This means that a normal recovered from a pixel could have a high accuracy on its azimuth and be very inaccurate on its elevation. As previously mentioned, this is due to the lighting condition variations over the image sequence.

We could leverage this information by performing regularization. Regularization is a type of prior we can put into the problem to favor some kind of solution over others. The kind of solution to be favored is chosen arbitrarily. In the case of object surfaces, a prior often made is the assumption of smoothness. This means that the optimization will be biased toward smooth solutions, i.e. not having abrupt curves. This is the prior that I will use for the rest of the section.

\todo{Add figure about reg.?}
Since we are on a 2D surface, two types of regularization exist: isotropic and anisotropic. Isotropic regularization means enforcing the same penalty for abrupt changes, regardless of the direction, whether the $x$ or the $y$ axis. Anisotropic regularization is the contrary: penalty won't be the same depending on axis the roughness, even for the same irregularity in the surface.

This idea was exploited in~\cite{hernandez-pami-11}, where they used first and second order anisotropic shape regularization. They worked on the case where the sequence of images is limited to three images, one of which is partially in shadow. This case led them to define the characteristic curve as the constraint induced by two lights, meaning a curve that has one degree of freedom. All the pixels lying on this curve are constrained in one direction (along the characteristic curve), but have no information on the other direction.
\todo{figure 3 of Hernandez}

In order to deal with this unknown direction, they choose to perform shape regularization. Without regularization, the results may be quite far from the ground truth~\ref{fig:her-2}. 
\todo{figure 2}

The regularization must be performed along directions that are perpendicular to the characteristic curves, let's call them $\mathbf{u}$. Hence, the regularization they use is of the form
\begin{equation}
\alpha \lvert \mathbf{u}^T \nabla z \rvert ^2
\end{equation}
for the first order, and 
\begin{equation}
\beta \lvert \mathbf{u}^T H(z)\nabla z \rvert ^2
\end{equation}
for the second order. Here, $z$ is, as before, the height, $\alpha$ and $\beta$ are the regularization weights and $H(z)$ is the Hessian matrix of $z$, its second-order derivatives. While the first order works to smooth the slopes, the second order enforces smooth curvature.

While the idea is similar, I am confronted to a problem which is a bit different. When the surface uncertainty is inferred directly from the lighting, it won't be purely in a single direction as described in Hernández \emph{et al.}. The uncertainty may be any combination of two orthogonal vectors, and may even end up with an isotropic behavior (i.e., the uncertainty is equal in both directions). As such, the $\mathbf{u}$ vector must be changed to $\mathbf{u}(x,y)$, with $x$ and $y$ being the coordinates in pixel-space.

We can simplify $\mathbf{u}(x,y)$ to a set of two orthogonal vectors along both axes. Since the uncertainty is computed directly on the $x$ and $y$ axes, we can rewrite the regularization as a linear combination of these two vectors:
\begin{equation}
\alpha \lvert \mathbf{u}_x \nabla z + \mathbf{u}_y \nabla z\rvert ^2 +
\beta \lvert \mathbf{u}_x H(z)\nabla z + \mathbf{u}_y H(z)\nabla z\rvert ^2
\quad,
\end{equation}
where $\mathbf{u}_x$ and $\mathbf{u}_y$ are the perpendicular components of the uncertainty.
\todo{In the case of edges?}

This new formulation is still untested. As such, my plan is to incorporate it into the reconstruction algorithm and to monitor its performance improvements.


\subsection{Selection and scaling}
% Discrepancy between simulation and real world captures. Investigate it. We witness: selecting some images of the sequence may improve or degrade dramatically reconstruction performance.
While working on the previous contributions, we witnessed a strange behavior in the reconstruction performance. Two symptoms were noticed, which -- we believe -- come from the same root, and will be solved by a single solution.

First, simulations on cloudy and semi-cloudy lighting conditions were really near the real captured data. But when the sun was directly visible by a surface patch, a drift in the intensity values was observed. This showed
\todo{show lines}

The second curious behavior detected can be easily deducted from the mathematical formulation. Sometimes, by adding images, the reconstruction uncertainty was increased, leading to worse results. At first, it may seem counter-intuitive: why would adding more data be harmful to the algorithm? The answer lies in the stability of the PS technique. As shown in the previous work, reconstruction uncertainty depends on three things:
\begin{enumerate}
  \item The sensor noise;
  \item The surface albedo;
  \item The stability of the lighting matrix.
\end{enumerate}
Over different sequences of images of the same lambertian object taken with the same photo camera, the sensor noise and the surface albedo does not change. The stability of the lighting matrix $L$-- the sequence of mean light vectors seen for a given surface -- will change, though. This means that the uncertainty will also change. As shown in our previous contributions, this uncertainty change is based on the singular values of the $L$ matrix. Adding repetitive or coplanar data to this matrix will increase the gap between the maximum and the minimum singular value. This has the effect of degrading the conditioning of the matrix, meaning that the stability will be impacted negatively. This translates in an increase of the uncertainty of the overall PS reconstruction. Conceptually, this means that the reconstruction algorithm will have an accuracy bias toward a certain direction.

Both of these issues touch two very important concepts:
\begin{enumerate}
\item Which information among the dataset is important;
\item How can they be improved.
\end{enumerate}
Not all data is equal, in that repetitive data is actually harmful for the algorithm. Then, which combination of input images should be kept? Which combination is optimal? What kind of preprocessing can we perform on the images to improve the result?

Using the condition number as a stability measure of the $L$ matrix, simply stacking images will sometimes increase or decrease the condition number, as can be seen in fig.~\ref{fig:}. But when ordering the input images by their contributions to define the singular vectors of the $L$ matrix (ranked from the most important to the least important) as shown in fig.~\ref{fig:}, we see that there's a clear optimal quantity of images. Past this quantity, the reconstruction quality is degraded.

\todo{figures condition value ordered and not ordered}

Even though this analysis shows promising results, it is based on the condition number and not a direct reconstruction performance assessment measure such as the uncertainty, as we used in our previous contributions. But even basing this analysis on the uncertainty won't give the best solution in our case. This analysis is static, meaning that it orders the input images regardless of the quantity we aim for. Maybe a completely different subset of images than the sorted one would be optimal for a certain quantity of input image.

To solve this problem, I propose to transfer some knowledge from the linear algebra and machine learning domains to PS. From the linear algebra domain, Golub \emph{et al.}~\cite{Golub1977} studied the rank degeneracy of matrices in the least squares problem. From the machine learning domain, a whole area of research called ``feature selection'' is based on a similar idea. Both of them have a similar goal: to select some columns of a matrix while optimizing some property, such as the condition number. In order to do so, the common way to perform this is by matrix factorization. I plan on reviewing these two fields of studies to gain a global understanding of the problem. This will allow me to use the uncertainty as measure instead of the condition number, and to modify an existing algorithm to solve this problem.
\todo{explain more}

Furthermore, once the optimal input images are selected, the conditioning of the problem may still be suboptimal. In that case, is there some processing that could be done on the images that could enhance the results? The bad conditioning can come from great variances in the lighting intensity. A common example is when the sun is shining at noon and a cloud occludes the sun in the afternoon. They both define a mean light vector (MLV) for a given normal. This MLV can potentially be very interesting vectors, but the intensity of the first one drowns the second. This means that the first vector will have an norm significantly higher than the second vector. When taken in the least squares sense, the first vector will have more importance when solving than the second vector. But in reality, we don't what this to happen; both are valid pixels gathered from the surface, explaining the first vector should be as important as explaining the second one.

This vector importance issue can be fixed by scaling the data by an arbitrary factor. By scaling both the input images and the computed MLV(s) to be of the same magnitude, the input images should be treated equally. Keeping in mind the impact of sensor noise, it is better to scale down the bright images than scaling up the dark images. The latter would boosting the sensor noise, giving inaccurate data, and thus a high reconstruction uncertainty.


\subsection{Shadow detection}
3DV15 article with cool SD


\subsection{Algorithmic improvements}

\todo{Is envmap defined before?}

% MRF, reference Jung's, tell our differences.
When performing a calibrated PS algorithm on environment maps, a chicken-and-egg problem arise. The part of the environment map visible by a surface patch is defined by its normal, but the normal is what we want to find using the lighting information. To solve this problem, as explained in our previous contributions, we can split a icosahedron three times to sample uniformly over the sphere a bunch of samples. Then, we can fit how well a pixel intensity in the image matches the MLV of all the sampled vectors in the environment map.

This scheme may work pretty well, but lacks a fundamental element: spatial coherence, which is very present in real world structures. As a way to enforce this, I propose to insert a new step to initialize the normals. To coarsely initialize the surface normals, the problem can be formalized as a Markov Random Field (MRF) by doing the hypothesis that a vertex normal is in some way related to its neighbors. This requires the definition of a unary and a pairwise term.

First, the unary term defines the cost of affecting a normal to a pixel. This unary term represents how well a normal explains the pixel intensity throughout the sequence of environment maps. This is done by computing the PS equation with the candidate normal and to compute its residual error. The residual error is the sum of all errors when predicting the pixel intensity from the MLVs over the image sequence. The higher this residual error is, the less the candidate normal explains well the pixel.

Then, the pairwise term defines the cost of a normal given its neighbors. A simple smoothness factor could be used, for example based on the angle between two evaluated normals. An interesting remark is made by~\cite{jung-cvpr-15}. Instead of the usual smoothness prior, one could base this pairwise terms on the likeness between pixels. They argue that pixels behaving similarly through the image sequence should also have similar surface normals. Technically, they base their similarity measure on the Pearson correlation coefficient.

The idea of using an MRF is to try all the normals of the icosahedron subdivided three times on all the pixels. This will give one residual error per candidate normal. All these residual errors compose the unary term of the MRF. The chosen pairwise weights explained earlier link neighboring pixels. This gives a MRF formulation that can be optimized using a standard library~\cite{Boykov2001a,Kolmogorov2004a,Boykov2004,Bagon2006}.

While this MRF formulation was used recently to perform PS reconstruction~\cite{jung-cvpr-15}, it has been used only used as a final smoothing step to fix zones in shadow during most of the sequence. I believe this technique can be employed to initialize the PS procedure, followed by the standard fine-grained normal optimization used by all PS algorithms.


% To do that, we'll need to merge some ideas from other techniques because sometimes, some pixels cannot be recovered. Silhouettes, Data-Driven priors.
The ideas previously conveyed will improve the reconstruction performance of outdoors PS and bring a better understanding of the impact of natural illumination on reconstruction algorithms based on photometric cues. But even if we improve a PS algorithm by a large margin, the analysis in our previous contributions showed that sometimes, even with the best possible surface with the lowest sensor noise, there is not enough photometric cues to recover certain normals or surfaces over the sphere.

To fix this issue, merging another technique with PS is the only choice. When data was captured on an unsuitable day, it will then be possible to still perform a 3D reconstruction, even in the uncertain zones. Previous work have shown that merging PS with Multi-View Stereo (MVS)~\cite{Beljan2012,Zhou2013,ackermann-3dv-14,HernandezEsteban2008,inose-tcva-13,shi-3dv-14} and even Structure from Motion (SfM)~\cite{zhang-iccv-03,lim-iccv-05} yield promising results. The difference between both technique is the calibration between the cameras. In Multi-View Stereo, the calibration of the cameras is known and the extrinsics between them is also known, while in Structure from Motion, the cameras are uncalibrated, which gives a projective ambiguity called the bas-relief ambiguity.

The idea I propose in the cases of unsolvable reconstructions using pure PS is to take some photos from another point of view, then performing a standard SfM algorithm to first get a coarse estimate of the scene. This will allow to fill the gaps with the dense result of PS that can constrain well on at least an axis in the worst case. For the other axis, the SfM result can be propagated to obtain a result.

\section{Uncalibrated outdoor reconstruction}
\label{sec:uncalib}

% reconstruction, don't need lighting condition is a plus
All the proposed work up until now supposes we know the full environment map that lit the scene we observe. After the development of the fully calibrated algorithm described, I plan to overcome the need for capturing the sky. Getting rid of the lighting capture is a big plus, allowing the technique to be easily brought on the field. A single camera is required, simplifying the capture apparatus.

Uncalibrated PS reconstruction has an issue that its calibrated counterpart doesn't have: the Generalized Bas-Relief Ambiguity (called GBR ambiguity in the literature). Because the light intensity and direction as well as the surface albedo is not known beforehand, the reconstruction of the surface height is up to an unknown factor, impossible to determine without prior knowledge of the surface or constrains external to photometric cues.

Authors over the years have basically proposed two ideas to surmount the GBR ambiguity. First, doing it manually (i.e., forcing a depth to an arbitrary value)~\cite{basri-ijcv-2007}. But most researchers proposed a wide amount of priors and optimization schemes on the object material~\cite{tan-cvpr-07,alldrin-cvpr-08,abrams-eccv-12,queau-jmiv-14} to solve this problem.

% Data-driven approach, what can the DB tell us on the image.
%Bas-Relief Amb.
In order to make PS work uncalibrated, I plan to use a data-driven approach. By taking advantage of the sky database we already have, we can lead an algorithm to learn what a physically plausible sky looks like, constraining greatly the problem. The quantity of real world skies that can explain well enough the illumination on a scene is quite small, if we take the similar skies as one.

I suspect this lighting constraint to be enough to alleviate the GBR ambiguity issue. In the case this constraint is not enough, I plan to use an optimization method on the material albedo as proposed by the authors previously mentioned.

Overall, the GBR ambiguity is just a minor issue mostly already solved in the literature. What is left to do, though, is an uncalibrated outdoor short term PS algorithm that brings performances similar to the laboratory conditions. The current state-of-the-art technique~\cite{jung-cvpr-15} obtain between 30\% and 40\% of the recovered normals under 30\degree of error, which is far away from the impressive precision obtained using PS in laboratory conditions.

In the following, I propose a succession of steps in order to take the calibrated algorithm proposed earlier and push it to a fully uncalibrated one.


\subsection{Half-calibrated case}
% First, reconstruction of the ground only.
A full environment map is hard to capture. This means that a full sphere of irradiance must be captured. Usually, sky capture apparatus is composed of a wide angle lens pointing toward up (zenith), capturing only at best a small portion of the horizon. Building a full environment map is done by adding the ground, which requires the stitching of two captures.

The first enhancement to the system I propose is to get rid of the ground and estimate it. As of now, in our previous contributions we estimate the ground to be a single constant, repeated over the bottom (nadir) hemisphere. This is a good enough approximation on objects that have been laid carefully on a uniform piece of cloth, but is likely to break on real building-scale structures.

One thing that can be done is an 


\subsection{Mostly uncalibrated}
% Then, estimation of the sky using the GPS coordinates and date & time. Then, without date and time.
In order to move toward an uncalibrated algorithm, it feels a logical next step to remove the environment map altogether from the inputs of the method. To simplify a bit this big step, I propose to work with the GPS coordinates alongside with the date and time. This type of information is sometimes available (and right) on the EXIF data of pictures, making this kind of input not as farfetched as it may seem.

With the GPS coordinates and the date and time, it is possible to know where the sun should have been in the sky. With this information, it is possible to determine whether the sun is occluded by clouds or not, depending on the intensity of the object image. If it is not occluded, the MLV of pixels not in shadow are constrained to be on a circle at a fixed elevation. This circle is due to the unknown object rotation. This elevation is found with the sun position.

By using previous work on finding the low frequency illumination~\cite{basri-ijcv-2007} or by using matrix factorization techniques~\cite{shi-cvpr-10}, it is possible to know the maximum light direction. If we have at least one image where the sun is shining onto the object, is it then possible to assign this maximum light direction to the sun, resolving the rotation ambiguity.

The sky could then be estimated by a two component parametric model of the sky, like~\cite{jung-cvpr-15} did. But I propose instead to take advantage of our sky database in order to either find or synthesize a new sky explaining the observed scene. This approach has the advantage of producing real-looking physically-based environment maps.

Synthesizing new skies can be a complex task. When talking about synthesizing, the meaning is basically to find an environment map that fits globally the required specifications and, at need, rotate it so the energy comes from the desired directions. More complex synthesizing could be interesting to test, for example generating a new environment map using texture synthesis, and will come if the need arise.

It is then possible to remove the time as an input of the method. Previously, we knew exactly the azimuth and elevation of the sun in the sky. Now, these values are constrained to an arc in the sky. I still assume the camera and object orientation are unknown. Now, using two images where the sun is not occluded, it is possible to solve this problem. Still using the maximum light direction technique, once two light directions are found, it is possible to find the camera azimuth (or object rotation). Using more images than two will provide an overconstrained system of equation, solvable using a standard least squares approach. As a bonus, the capture time can also be estimated.


\subsection{Fully uncalibrated}
% Lastly fully uncalibrated
The last step is to provide a fully uncalibrated PS algorithm. Building on the mostly uncalibrated technique, it is possible to remove the GPS coordinates and the date from the technique. By reusing the scheme of finding the maximum light direction, it is possible to synthesize a sequence of skies with the desired intensity coming from any direction. It is expected that at least some captures over the sequence will be sunlit, yielding a high overall image intensity and giving a clear principal light direction. By fitting these principal light directions, it is possible to find the sun arc in the sky, allowing a self-calibration.

This proposed technique will be a major contribution that iterates over the previous state of the art by adding a richer data-driven sky illumination model.


\section{Looking farther than the sun}

% Up until now, we only considered the atmosphere during the day for reconstruction. The main problem, coplanar.
All the previous proposed work only considered the atmosphere during the day. The main problem, which is the coplanarity of the sun trajectory over the course of a day, can be solved by richer and more complex sky models. Another interesting avenue is to revert to the simple case: the point light sources. In this case, using solely the sun as a light source will, most of the time, give an underconstrained problem that will not perform well.

Keeping this analysis, though, it is possible to bring something else into play in order to better constrain the problem. It is hard to find something capable of competing with the sun in terms intensity in the sky through a day, but the moon can up to par in terms of quantity of information needed to constrain the problem correctly. Another avenue is to manually modify the lighting, either by distorting the sun light or adding new light sources.


\subsection{Day \& Night PS}
When seen during the day, the moon doesn't bring much lighting to a scene. But during the night, it can be strong enough to cast clearly visible and sharp shadows on the ground. But the main interesting thing about the moon is that is presents lighting from a plane different than the sun. Using this to our advantage could turn a data capture failure for PS (e.g., during a fully sunny day) into a successful reconstruction, by simplifying the illumination model and letting the capture continue overnight.

Over the course of 24 hours, the sun and moon paths in the sky are each mostly coplanar. Examples of such trajectories are shown in fig.~\ref{fig:DN-sunmoon-trajectories}. Both of them are parallel, which is not surprising because the earth, the sun and the moon are roughly on the same plane in the solar system. But the inclination of the earth makes both trajectories superposed in the sky during the equinoxes, which occurs around 81 days (spring) and 264 days (fall) after January 1st.

\begin{figure}
\centering
\begin{tabular}{cc}
\includegraphics[height=0.49\linewidth]{DayNight/20150225_lat47.png} &
\includegraphics[height=0.49\linewidth]{DayNight/20150225_lat75.png} \\
a) & b)
\end{tabular}
\caption{Example of the trajectories of the sun and the moon for 2015-02-25 at latitude a) 47 and b) 75. Two things are interesting to note: First, the moon trajectory suddenly stops because at this moment the sun rises. The moon is then not considered as a dominant light source. Secondly, as the latitude gets higher, the trajectories becomes more and more circular.}
\label{fig:DN-sunmoon-trajectories}
\end{figure}

The measure used to analyze the PS reconstruction potential using both days is the maximum gain. This maximum gain is the factor that multiplies the sensor noise, as explained in the previous contributions. The higher it is, the more uncertain (or wrong) the reconstruction will be. So the lower is the maximum gain, the better will be the reconstruction performance.

Such an analysis for year 2015 is found in fig.~\ref{fig:DN-maximum_gain} for the University Laval location and fig.~\ref{fig:DN-maximum_gain_latitude} \todo{for most of earth latitudes}. As we can see, the two major peaks around days 81 and 264 are explained by the equinox trajectory alignment. Since both trajectories are almost superposed, the system of equations is pretty badly conditioned, leaving a high maximum gain. Another interesting thing to notice is the small sinusoid with a 29.54 days period present over the whole year. When not much of the moon is visible during a night, for example it may be rise at 4 am. Since the sun's dawn is pretty near, the moonlit duration is small, giving only a small arc of the moon plane. When this happens, the maximum gain increases. The increase is not so bad when far from the equinoxes, because this analysis supposes at least one capture is available with the moon, reducing the problem to a matrix of coplanar vectors with the addition of (at least) a single vector outside of this plane, which is enough to constrain the problem.

\begin{figure}
\centering
\includegraphics[height=8cm]{DayNight/maximum_gain.pdf}
\caption{Maximum gain for PS reconstruction assuming an illumination of 24 hours (sun and moon) for the year 2015. The color-coded moon illumination represent the phase of the moon, full moon being 1 and new moon being 0. The location is the University Laval Campus (Latitude and Longitude = 46.779077, -71.275778 ). }
\label{fig:DN-maximum_gain}
\end{figure}

%\begin{figure}
%\centering
%\includegraphics[height=8cm]{DayNight/maximum_gain_over_latitude.png}
%\caption{Maximum gain in function of the day for the year 2015 and the latitude of the observer. This gain is clipped to 1.}
%\label{fig:DN-maximum_gain_latitude}
%\end{figure}

Another notion worth of mention is the moon illumination. Over the course of its cycle, the moon will have an illumination of
\begin{equation}
k = \frac{1 + \cos(\phi)}{2}  \quad,
\end{equation}
where $\phi$ is the lunar phase in radians~\cite{Meeus1991}. While the technique will obviously not work when the illumination is null, the rest of the time, the reconstruction performance will be highly dependent upon the light pollution at the capture location. This light pollution can be approximated by an ambient illumination term, which is analyzed by~\cite{Angelopoulou2013}.

Statues, historical bridges and architectural monuments are often lit using spotlights during the night.

The main problem with this idea, as can be seen from the unusual astronomical arguments made in this section, is the knowledge transfer between the astronomical domain and computer vision. We can assume that most of the people aware of PS have little to no knowledge of the side effects of the cycle of the moon and its trajectory in the sky. By investigating this field of study and summarizing the pertinent informations, I am certain that performing day \& night PS will bring a nice contribution to the community.


%\subsection{Artificial alterations}

%The sun, when taken as a point light source, may have a coplanar trajectory over the course of a day, but nothing prevents the addition or modification of this lighting.

% Diffusers

% City lights


%\subsection{Other techniques}



%\section{Augmentation avec d'autres techniques}

% from ICCP
%While these techniques can lead to well-defined solutions, they are not always practical in many scenarios with strict temporal or geographical constraints. A second strategy has therefore been to combine PS with other techniques such as multi-view stereo~\cite{inose-tcva-13,shi-3dv-14}, or use reference objects as in \cite{johnson-cvpr-11} or example-based PS~\cite{hertzmann-pami-05,ackermann-3dv-14}. But can we accurately reconstruct surface geometry simply based on the photometric cue in an outdoor setting, without overly restrictive temporal and geographical constraints?

%Décrire l'amélioration que pourrait apporter la PS utilisée conjointement avec du SFM et la stéréo multivues standard.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Schedule}

\todo{Add scheduled contributions.}

%The problems will be explored in increasing order of complexity according to the following schedule. First, the hardware required to gather the sky illumination database will be setup on a tall roof nearby, and the data acquisition process will be continued over a period of several months to build a rich dataset. Second, experiments with existing PS techniques to reconstruct shape with known illumination will be conducted. Leaning on these experiments, the algorithms will then be adapted to require knowledge only of the sun direction, obtained from the capture timestamps. Finally we will experiment with images where the lighting conditions are unknown.

\begin{itemize}
	\item 2016h: Selection and Calibrated PS algorithm
	\item 2016e: Capture, Blind PS algorithm
	\item 2016a: Blind PS algorithm, Day \& night
	\item 2017h: Fusion with SFM
	\item 2017e: Capture, Fusion with Multiview Stereo Techniques
	\item 2017a:
	\item 2018h: Thesis writing and defense.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}\label{conclusion}

Recap.

\chapter*{Acknowledgments}

Mathieu Garon
Julien Becirovski
Paulo F.U. Gotardo

\chapter*{Annexes}

\section{Permutations of pair of images}
\label{anx:permutations}

From a set of $n$ images, the quantity of possible pairs of images is expressed as a special case of the permutation without repetition:
\begin{equation}
\frac{n!}{\left( n - 2 \right)!} \quad.
\end{equation}

By the definition of the factorial, this equation can be simplified to
\begin{equation}
\frac{n \times (n-1) \times (n-2) \times (n-3) \times \cdots \times 2}{(n-2) \times (n-3) \times \cdots \times 2} \quad.
\end{equation}

By simplifying, we find that the quantity is exactly
\begin{equation}
n^2 - n\quad,
\end{equation}
giving a quadratic expression (also called $\mathcal{O}(n^2)$ in Bachmann-Landau notation).

{\small
\bibliographystyle{acm}
\bibliography{library}
}

\end{document}
