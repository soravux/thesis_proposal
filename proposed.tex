% !TEX root = main.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Proposed contributions}

% Based on the work already done
Previous contributions mainly focused on the analysis of the complex illumination conditions of outdoor scenes. This analysis brought us a good deal of knowledge that will enable the enhancement of existing reconstruction algorithms.

But first, something is missing among the community to continue experiments with outdoor PS: a database of diverse objects publicly available. Just as the sky database we built, I propose to capture and share unsaturated HDR images of objects under various natural illumination conditions.

Then, using this database, I propose to modify existing PS algorithms to work for outdoor conditions. This work will be split in steps: first, I will tackle the case when the illumination is known. Second, I will focus on gradually moving toward the uncalibrated, that is the case where only some object images are required. This includes merging ideas from other 3D reconstruction techniques to simplify the problem or correct some issues that PS is not able to fix in the general case. Third, after having improved existing PS reconstruction algorithms using knowledge from the day lit sky, I will propose to use the moon as another light source to better constrain the problem when a day is fully cloudy or sunny.


\section{Database of objects}
\label{sec:dbobj}

As explained in \ref{sec:hdrdb}, we've built a publicly available database of unsaturated HDR sky captures. This database filled a need for a publicly available unsaturated HDR natural illumination conditions, enabling physically-based renderings and sky analysis. There is now a need for a complementary dataset: images of objects usually lit by the sky. Objects such as statues, architectural monuments and buildings. This database would allow researchers to analyze the shading of surfaces in conjunction with their illumination conditions. The goal is to obtain and share pictures of objects synchronized with the sky captures. These images will give a direct link between the aspect of a sky and the appearance of objects lit by it, effectively tying natural illumination with its shading.

% Cite AMOS http://amos.cse.wustl.edu/
Databases of outdoor scenes already exist in the community (e.g., AMOS~\cite{jacobs-cvpr-2007}, Yahoo!'s databases~\cite{thomee-arxiv-15}), leading one to believe that this kind of data is wanted and appreciated by the researchers. Data available yields a lot of different content and objects with various reflectances. The issue of this data, for this thesis, is the lack of information about lighting conditions. A full HDR capture of the sky synchronized with the images is not available. Furthermore, it is often difficult to estimate what the illumination looked like. Images may, or may not, have GPS coordinates, sometimes with the wrong information. Date and time in the EXIF data of the images are often wrong or skewed. Camera orientation (such as azimuth) is mostly never provided with these databases. As such, it is then hard to guess the position of the sun in the sky. These databases are really interesting to test an uncalibrated reconstruction technique, but sky environment maps synchronized to the scenes captures are missing for its development.

% small & big
The idea of the database would be to gather images of various objects. Simple matte, Lambertian surfaces will be the focus since all the light from the hemisphere of visibility is treated equally, making them simpler to work with. Relatively big objects such as buildings and statues are the main subjects of this database, as they are usually sitting outside, lit by natural illumination. No other technique than PS allows their scanning while being inexpensive. But for the development of an algorithm, the ground truth of the captured objects are necessary. Scanning building-scale with an handheld 3D scanner to obtain its ground truth is pretty impracticable and tedious, though. As such, capturing small objects such as little statuettes or trinkets is the first thing to do, because it is possible to easily obtain a ground truth mesh from them using a handheld 3D scanner. Having the data ready to perform the rest of the project is a logical step in the right direction. This database is the answer to this need.

% Build on existing infrastructure
A lot of effort is required to build and share a new dataset. This is why I plan on reusing the infrastructure we already have for the sky database. The capture system is already available, there is only one thing left to do: taking the captures. The goal is to be as efficient as possible and focus on the objects to be captured.

\begin{figure}
    \centering
    \includegraphics[width=.52\linewidth]{./figures/realData/realData-setup.pdf} \\[1mm]
    \caption{Small objects capture setup. HDR photographs of the sky and of the object (owl statuette) are simultaneously captured by two cameras installed on the roof of a tall building.}
    \label{fig:proposed-real-data-setup}
\end{figure}


% Technical side
On the technical side, a Canon 5D mark III camera will be used in addition to the system already in place for the sky capture described in \ref{sec:hdrdb}. This additional camera will perform 5 burst captures of the object, in order to gather all the information needed to build an unsaturated HDR photo. A small chrome sphere is inserted in this camera's field of view to allow calibration with the sky camera. Hence, it is possible to compute the precise rotation between both cameras by projecting the image . A system based on ROS developed by interns\footnote{Available publicly at \url{https://github.com/lvsn/CameraNetwork}} orchestrates the camera network and synchronizes the captures among cameras. Fig.~\ref{fig:proposed-real-data-setup} presents the whole system in action.


\section{Calibrated outdoor reconstruction}
\label{sec:calib}

 % Up to now, only analysis. Make something with it.
Until now, the work done in the previous contributions revolves around the analysis of natural skies. The next step is to use this new knowledge to build a reconstruction algorithm. Here are described the steps I intend to achieve first, followed by the insights that will make it work.

% Calibrated case.
Here is the list of assumptions made in order to build a calibrated outdoor PS reconstruction algorithm. The whole sky is assumed to be known to the algorithm. This means that the Mean Light Vectors (MLVs) matrix $\bar{L}$, as defined in \ref{iccp15:ifm}, can be known a priori, given the right normals. Object surfaces are assumed to be Lambertian. They are however not required to yield the same albedo over the whole surface.

The images of the sequence must all be aligned. Image alignment or registration is another field of study which is solved for the case useful for PS. This is discussed in~\cite{ackermann-cvpr-12}, as explained in \ref{sota:ops-months}. I will suppose the images were already aligned using such technique in the proposed algorithm.

% Explain the DLT
In the basis algorithm I plan to build upon, there are two important enhancements from the original PS algorithm as defined in \ref{sec:ps_ori}. First, it is possible to remove the albedo $\rho$ from the equation to cancel the effect of the surface reflectance, making the algorithm robust to albedo variability. Second, instead of solving for the normals, it is possible to solve for the surface height directly. 

To remove the albedo $\rho$ from the original PS equation, the trick is to divide an image by another one. Mathematically, this pixelwise division of, say, image 1 by image 2, is represented as

\begin{equation}
\frac{b_{p,1}}{b_{p,2}} = \frac{\rho_p \mathbf{l}_1 \mathbf{n}_p}{\rho_p \mathbf{l}_2 \mathbf{n}_p} \quad,
\end{equation}
for pixel $p$. The albedo $\rho_t$ cancels out, giving the relation
\begin{equation}
\label{eq:ratio_images}
\left( b_{p,1} \mathbf{l}_2 - b_{p,2} \mathbf{l}_1 \right) \mathbf{n}_p = 0  \quad.
\end{equation}

This relation was used before~\cite{yu-iccp-13,wu-pami-06}. This kind of algorithm usually consists of taking a denominator image minimally affected by shadows and highlights and dividing all the other images of the sequence by this one. While this removes the constant albedo assumption, it leaves a smaller (by one) sequence of images to perform PS. Another possibility is to perform the division on all the possible permutations of images, giving a better robustness to the algorithm. The problem with this approach is that it makes the sequence grow quadratically, which quickly becomes problematic to solve in a reasonable time on computers having a finite amount of memory. Stochastically taking a predetermined amount can do the trick, but optimizing the conditioning of the $\bar{L}$ matrix .
 
Solving directly for the surface height instead of the normals can be done by analyzing the relation between both. Given pixel $p$ and its surface gradient $z_p$, the normal of the surface patch $\mathbf{n}_p$ can be expressed as
\begin{equation}
\hat{\mathbf{r}}_p = 
\begin{bmatrix}
z_{p,x} \\
z_{p,y} \\
-1
\end{bmatrix}
\end{equation}
\begin{equation}
\mathbf{n}_p =
\frac{\hat{\mathbf{r}}_p}{\norm{\hat{\mathbf{r}}_p}} \quad.
\end{equation}
This supposes that the surface is pointing toward $-1$, hence the camera is orthographic and on the axis $[0, 0, -1]$.

When merged with the albedo removal technique (equation \eqref{eq:ratio_images}), we obtain
\begin{equation}
\label{eq:pre-dlt}
\left( b_{p,1} \mathbf{l}_2 - b_{p,2} \mathbf{l}_1 \right)
\begin{bmatrix}
z_{p,x} \\
z_{p,y} \\
-1
\end{bmatrix}
= 0  \quad.
\end{equation}

This version of PS is the base upon which the proposed enhancements are planned. In order to make it easier to handle, a Direct Linear Tranform (DLT) method is applied to rewrite equation~\eqref{eq:pre-dlt} into an set of homogeneous linear equations.

To do so, we must consider the left part of the equation, $\left( b_{p,1} \mathbf{l}_2 - b_{p,2} \mathbf{l}_1 \right)$, as being a vector of three scalars, let's say $\left[ i_p, j_p, k_p \right]$. When considering the division of an image $a$ by $a'$ as a whole instead of looking at each pixels separately, we get the vectors $\left[ \mathbf{i}_{aa'}, \mathbf{j}_{aa'}, \mathbf{k}_{aa'} \right]$. Equation \eqref{eq:pre-dlt} can now be rewritten as
\begin{equation}
\left[ \mathrm{diag}(\mathbf{i}_{aa'}) \quad \mathrm{diag}(\mathbf{j}_{aa'}) \quad \mathbf{k}_{aa'}\right]
\begin{bmatrix}
\mathbf{z}_{x} \\
\mathbf{z}_{y} \\
-1
\end{bmatrix}
= 0 \quad,
\end{equation}
which can be rewritten as
\begin{equation}
\label{eq:diagdlt}
\left[ \mathrm{diag}(\mathbf{i}_{aa'}) \quad \mathrm{diag}(\mathbf{j}_{aa'}) \right]
\begin{bmatrix}
\mathbf{z}_{x} \\
\mathbf{z}_{y} \\
\end{bmatrix}
= \mathbf{k}_{aa'} \quad.
\end{equation}

By multiplying both sides of \eqref{eq:diagdlt} by $\begin{bmatrix}\mathrm{diag}(\mathbf{i}) & \mathrm{diag}(\mathbf{j}) \end{bmatrix}^T$, we obtain the normal equations
\begin{equation}
\begin{bmatrix}
    \mathrm{diag}(\mathbf{i} \circ \mathbf{i}) && \mathrm{diag}(\mathbf{i} \circ \mathbf{j}) \\
    \mathrm{diag}(\mathbf{i} \circ \mathbf{j}) && \mathrm{diag}(\mathbf{j} \circ \mathbf{j}) \\
\end{bmatrix}
\begin{bmatrix}
\mathbf{z}_{x} \\
\mathbf{z}_{y} \\
\end{bmatrix}
=
\begin{bmatrix}
\mathrm{diag}(\mathbf{i} \circ \mathbf{k}) \\
\mathrm{diag}(\mathbf{j} \circ \mathbf{k}) \\
\end{bmatrix} \quad ,
\end{equation}
where $\circ$ denotes the pairwise (Hadamard) product.

A variable $\mathbf{M}$ is then needed to integrate the estimated gradients $z_p$ into a surface $\mathbf{z}$. This $\mathbf{M}$ variable is split into two components, $\mathbf{M}_x$ and $\mathbf{M}_y$, representing the neighboring relationship, or the derivatives, on the $x$-axis and $y$-axis, respectively:
\begin{equation}
\label{eq:M}
\mathbf{M}_x = 
\begin{bmatrix}
\ddots &    & \ddots &   &        \\
       & -1 &        & 1 &        \\
       &    & \ddots &   & \ddots
\end{bmatrix}
\mathbf{z} = \mathbf{M}_x\mathbf{z}  \quad.
\end{equation}
Note that, to simplify the notation, $\mathbf{z} = \mathrm{vec}(\mathbf{Z})$.

For a boundary pixel, one can simply consider that its $x$-derivative is equal to that of its neighbor, thus enforcing a planarity constraint at the surface boundary. The definition of $\mathbf{M}_y$ is analogous to $\mathbf{M}_x$.

In order to convert this independent systems of linear equations into an homogeneous form, it is possible to arrange this matrix into a block diagonal matrix by taking each row and making $2 \times 2$ blocks from them.
This gives this new formulation:
\begin{equation}
\sum_{a,a'}
\begin{bmatrix}
    D_{ii} & D_{ij} \\
    D_{ij} & D_{jj} \\
\end{bmatrix}_{a,a'}
\begin{bmatrix}
    \mathbf{M}_x \\
    \mathbf{M}_y \\
\end{bmatrix}
\mathbf{z} =
\sum_{a,a'}
\begin{bmatrix}
D_i \mathbf{k} \\
D_j \mathbf{k} \\
\end{bmatrix}_{a,a'}
\quad,
\end{equation}
where $D_i = \mathrm{diag}(\mathbf{i}_{aa'})$ and $ D_{ij} = \mathrm{diag}\left(\mathbf{i}_{aa'} \circ \mathbf{j}_{aa'}\right)$ ($\circ$ being the elementwise, or Hadamard, product). This gives the DLT formulation of the PS problem with the two proposed enhancements: the albedo invariance trick and the direct surface estimation.

% Normalization
%This formulation of PS has a problem. Since the first line is multiplied by $D_i$ and the second by $D_j$, the conditioning of this system is then also squared, which is not desirable. 

% What's coming up next
Four avenues will be discussed on how I plan to make this PS algorithm work for outdoors. First, using the observations we made in our analysis brought us some ideas on how to improve the reconstruction performance. Two of such ideas are discussed. Then, interesting elements found in other algorithms and their inclusion in the proposed algorithm will be explained.

\subsection{Controlling uncertainty using anisotropic regularization}
\label{subsec:reg}

% One thing we noticed during our analysis is that not all pixels nor axis are equal.
% Figure, axis uncertainty
During our analysis of sec.~\ref{sec:iccp15-datasetanalysis}, we noticed that not all pixels have the same uncertainty. Certain portions of the sphere were more prone to exhibit erratic reconstructions than others. Moreover, we found that a single pixel would often have a different uncertainty depending on its normal direction. This means that a normal recovered from a pixel could have a high accuracy on its azimuth and be very inaccurate on its elevation. This is a direct effect of natural lighting conditions variations over the image sequence.

We propose to leverage this information by performing regularization. Regularization is a type of prior we can put into the problem to favor some kind of solution over others. In the case of object surfaces, a prior often made is the assumption of smoothness. This means that the optimization will be biased toward smooth solutions, i.e.\ not having sharp discontinuities. This is the prior that will be used for the rest of the section.

Two types of regularization exist: isotropic and anisotropic. Isotropic regularization means enforcing the same penalty for abrupt changes regardless of the direction, whether on the $x$ or the $y$ axis. Anisotropic regularization is the opposite: penalty won't be the same depending on which axis the roughness lies. The same irregularity may give a completely different penalty depending on which axis it lies on.

This idea was exploited in~\cite{hernandez-pami-11}, where they used first and second order anisotropic shape regularization on PS. They worked on the case where the sequence of images is limited to two images, or analogously three image with one of which is partially in shadow. This case led them to find that the constraint induced by two lights gives surface gradient information in a direction, but has no information in the orthogonal direction, leaving a degree of freedom in the recovered normals. Tracing along this direction of known gradient gives a curve which the authors call the characteristic curve. An example of such curve is shown in fig.~\ref{fig:reg-her3} (b). All the pixels lying on this curve are constrained in one direction (along the characteristic curve), but have no information on the other direction, giving possible reconstruction artifacts as shown in fig.~\ref{fig:reg-her3} (c).

\begin{figure}
\centering
\includegraphics[height=8cm]{Regularization/hernandez_fig3.png}
\caption{Two-source photometric stereo with varying albedo. \textbf{(a)} shows the two input images. \textbf{(b)} shows the characteristic curves obtained by plotting the seeds following the 2D flow. \textbf{(c)} shows one possible reconstruction. Note how each curve is reconstructed independently because of a lack of constrain across neighboring curves. \textbf{(d)} and \textbf{(e)} shows reconstruction using anisotropic regularization, with $\alpha = 0.1, \beta = 0$ and $\alpha = 0, \beta = 0.5$, respectively. Figure from~\cite{hernandez-pami-11}}
\label{fig:reg-her3}
\end{figure}

In order to deal with this undesired extra degree of freedom, they choose to perform shape regularization. Without regularization, the results may be quite far from the ground truth as shown in fig.~\ref{fig:reg-her2}.

\begin{figure}
\centering
\includegraphics[height=8cm]{Regularization/hernandez_fig2_abc.png}
\caption{Experiment on shadowed input. \textbf{(a)} input images. \textbf{(b)} Ground truth, reconstruction performed without shadows. \textbf{(c)} Reconstruction performed with the shadows. The lighter zones represents the shadows. Figure from~\cite{hernandez-pami-11}}
\label{fig:reg-her2}
\end{figure}

In the case of two-source PS, the regularization must be performed along directions that are perpendicular to the characteristic curves, let's call them $\mathbf{u}$. Hence, the regularization they use is of the form
\begin{equation}
\alpha \lvert \mathbf{u}^T \mathbf{z} \rvert ^2
\end{equation}
for the first order, and 
\begin{equation}
\beta \lvert \mathbf{u}^T \nabla \mathbf{z} \; \mathbf{u} \rvert ^2
\end{equation}
for the second order, where $\mathbf{z}$ is the gradient of the surface height $\iint_S\mathbf{z} \: dS$. $\alpha$ and $\beta$ are the regularization weights and $\nabla \mathbf{z}$ is the Hessian matrix of the surface height (its second-order derivatives). While the first order regularization penalizes non-smooth the slopes, the second order penalizes non-smooth curvatures.

While the idea is similar, the problem of regularizing as a function of the normal uncertainty as we propose to do here is a bit different. When the pixel normal uncertainty is inferred directly from the lighting, it won't be in a purely single direction as in the case of \cite{hernandez-pami-11}. The uncertainty may be any combination of two orthogonal vectors, and may even end up having an isotropic behavior (i.e., the uncertainty is equal in both directions). As such, there are now both vectors $\mathbf{u}_1$ and $\mathbf{u}_2$, representing two different directions on which regularization is performed.
%\begin{equation}
%\mathbf{U} = 
%\begin{bmatrix}
%    \mathbf{u}_1 \\
%    \mathbf{u}_2 \\
%\end{bmatrix} \quad ,
%\end{equation}

Rewriting $\mathbf{u}_1 = \begin{bmatrix} u_1 & v_1\end{bmatrix}$, it is possible to derive both first-order and second order regularization as
\begin{equation}
\alpha \left\lvert \begin{bmatrix} u_1 & v_1 \end{bmatrix} \begin{bmatrix} z_x \\ z_y \end{bmatrix} \right\rvert ^2 + 
\beta \left\lvert \begin{bmatrix} u_1 & v_1 \end{bmatrix} \begin{bmatrix} z_{xx} & z_{xy} \\ z_{xy} & z_{yy} \end{bmatrix} \begin{bmatrix} u_1 \\ v_1 \end{bmatrix} \right\rvert ^2 \approx 0
\quad,
\end{equation}
from which we obtain, after the matrix multiplications, 
\begin{equation}
\alpha \left\lvert u_1\cdot{}z_x + v_1\cdot{}z_y  \right\rvert ^2 + 
\beta \left\lvert  u_1^2\cdot{}z_{xx} + 2u_1v_1\cdot{}z_{xy} + v_1^2\cdot{}z_{yy} \right\rvert ^2
\approx 0
\quad.
\end{equation}
Given the regularization directions for the whole image $\mathbf{U} = \begin{bmatrix}\bar{\mathbf{u}} & \bar{\mathbf{v}} \end{bmatrix}$, with $\bar{\mathbf{u}}$ and $\bar{\mathbf{v}}$ being respectively column vectors of all the $x$-axis and $y$-axis regularization directions for each pixel, and using the notation developed in \eqref{eq:M}, we obtain
\begin{equation}
\alpha \left\lvert
\begin{bmatrix} \mathrm{diag}(\bar{\mathbf{u}}) & \mathrm{diag}(\bar{\mathbf{v}}) \end{bmatrix}
\begin{bmatrix}
M_x \\
M_y
\end{bmatrix}
\mathbf{z}
\right\rvert ^2 + 
\beta \left\lvert
\begin{bmatrix} \mathrm{diag}(\bar{\mathbf{u}} \circ \bar{\mathbf{u}}) &  2\cdot{}\mathrm{diag}(\bar{\mathbf{u}} \circ \bar{\mathbf{v}}) & \mathrm{diag}(\bar{\mathbf{v}} \circ \bar{\mathbf{v}})
\end{bmatrix}
\begin{bmatrix} M_{xx} \\ M_{xy} \\ M_{yy} \end{bmatrix}
\mathbf{z}
\right\rvert ^2
\approx 0
\quad.
\end{equation}
This equation gives the regularization penalty, a non-smoothness measure along directions $\mathbf{U}$ (one row per pixel) of the solution. Performing this equation a second time using another regularization matrix $\mathbf{U}_2 = \begin{bmatrix}\bar{\mathbf{u}}_2 & \bar{\mathbf{v}}_2 \end{bmatrix}$ gives the penalty along these new vectors. Summing the result gives the regularization for both vectors.

This anisotropic regularization is interesting because it offers a per-pixel regularization over multiple different directions. The strength over any direction can be set independently for every pixel of the image to reconstruct.

This new formulation is untested in a PS reconstruction algorithm. As such, my plan is to incorporate it into the reconstruction algorithm and to monitor its performance improvements. Something that is not taken into account as of now are the edges. Two pixels having two completely different intensities over the image sequence indicate that their normals should be quite far apart and thus should not be strongly regularized for smoothness, as noticed by~\cite{jung-cvpr-15}.


\subsection{Selecting the right images to better constrain the problem}
\label{subsec:selection}
% Discrepancy between simulation and real world captures. Investigate it. We witness: selecting some images of the sequence may improve or degrade dramatically reconstruction performance.

%First, simulations on cloudy and semi-cloudy lighting conditions reflected correctly the real captured data. But when the sun was directly visible by a surface patch, a drift in the intensity values was observed compared to the theory. Pixels seeing direct bright sunlight were typically dimmer than what the Lambertian equation predicted, as can be seen in fig.~\ref{fig:sel-scaling_error}. This situation is corroborated by the real data. In itself, this minor setback won't hinder the forecast work. It seems to suppose that our real objects does not exhibit perfectly Lambertian reflectances, which is not quite surprising, and is something that the physically-based renderer LuxRender also picks up. The important thing to keep in mind from this is that images lit by direct sunshine will impact negatively the performance of the PS algorithm.

%\begin{figure}
%\centering
%\includegraphics[height=8cm]{selection_scaling/intensity_sphere_lux_lin.png}
%\caption{Median intensity of rendered images. The physically-based renderer LuxRender ($y$-axis) is compared against the Lambertian equation ($x$-axis). The same material and light directions are used in both rendering methods. A linear best fit is shown in red. As it can be seen, the curve is not perfectly linear.}
%\label{fig:sel-scaling_error}
%\end{figure}
While working on the previous contributions, we witnessed a curious behavior in the reconstruction performance. Sometimes, by adding images, the reconstruction uncertainty was increased, leading to worse results. At first, it may seem counter-intuitive: why would adding more (non-outlier) data be harmful to the algorithm? This curious behavior can however be deducted from the stability of the PS mathematical formulation. As shown in sec.~\ref{iccp15:ifm}, reconstruction uncertainty depends on three things: the sensor noise, the surface albedo and the stability of the lighting matrix.

Over different images of the same Lambertian object taken with the same photo camera, the sensor noise and the surface albedo does not change. The stability of the lighting matrix $\bar{\mathbf{L}}$---the sequence of MLVs seen for a given surface---will change, though. This means that the uncertainty will also change, since it is based on the singular values of the $\bar{\mathbf{L}}$ matrix. Adding repetitive or coplanar data to this matrix will increase the difference between the maximum and the minimum singular value. This relationship can be deduced by the Gramian matrix of $\bar{\mathbf{L}}$, chiefly $\mathbf{G} = \bar{\mathbf{L}}^T\bar{\mathbf{L}}$. This Gramian matrix is now square, but its singular values are the squared singular values as the original matrix, $\sqrt{\sigma_i(\mathbf{G})} = \sigma_i(\bar{\mathbf{L}})$. By computing this Gramian matrix, the repeated vector multiplied (or projected) multiples times the other vectors, thus having an higher importance in the Gramian matrix, We could say that the Gramian matrix was steered toward this vector. As such, an higher component 

This has the effect of degrading the conditioning of the matrix, as $\kappa(\bar{\mathbf{L}}) = \frac{\sigma_{\max}(\bar{\mathbf{L}}) }{ \sigma_{\min}(\bar{\mathbf{L}}) }$, meaning that the stability will be impacted negatively. It translates in an increase of the uncertainty of the overall PS reconstruction. Conceptually, this means that the reconstruction algorithm will have a stability bias toward the direction most present in $\bar{\mathbf{L}}$.

Both of these issues touch a very important concept: which images among the whole dataset are important. Not all data is equal, in that repetitive data is actually harmful for the algorithm. Then, which input images should be kept? Which combination is optimal?

Using the condition number as a stability measure of the $\bar{L}$ matrix, simply stacking images will sometimes increase or decrease the condition number, as can be seen in fig.~\ref{fig:sel-ordering}. But when ordering the input images by their contributions to define the singular vectors of the $\bar{L}$ matrix (ranked from the most important to the least important) as shown in fig.~\ref{fig:sel-ordering}, we see that there's a clear optimal quantity of images. Past this quantity, the reconstruction quality is degraded.

%\todo{Add a figure with a reconstruction with and without the selection as of now.}

\begin{figure}
\centering
\begin{tabular}{ccc}

\includegraphics[width=0.32\linewidth]{selection_scaling/example_MLV.png} &
\includegraphics[width=0.32\linewidth]{selection_scaling/bytime.pdf} &
\includegraphics[width=0.32\linewidth]{selection_scaling/byspan.pdf} \\
(a) & (b) & (c)
\end{tabular}
\caption{\textbf{(a)} Evolution of Mean Light Vectors (MLV) over a day. \textbf{(b)} Condition number of the $\bar{L}$ matrix when taking the images ordered by time. The $x$-axis represents the number of images taken. \textbf{(c)} Same, but with images ordered by their importance, i.e., how their MLV span the space. Note how taking the whole sequence gives the same condition number, but the sequence in which they are presented can increase the condition number up to twice the maximum value obtained when ordered by acquisition time.
\label{fig:sel-ordering}
\end{figure}

Even though this analysis shows promising results, it is based on the condition number and not a direct reconstruction performance assessment measure such as the uncertainty, as we used in sec.~\ref{iccp15:ifm}. But even basing this analysis on the uncertainty won't give the best solution. This analysis is static, meaning that it simply orders the input images. The quantity of images is also an important factor to consider. Too many input images would yield useless repetitive data, so the goal is to minimize the quantity of images down to 3 in the theoretical case. In our case, the albedo elimination trick requires 4 images minimum to work. Adding more images than 3 is a good idea, though, as it brings robustness to noise and outliers. Furthermore, maybe a different combination of 5 images may yield a $\bar{\mathbf{L}}$ matrix that spans the space better than the optimal combination of 4 images. The optimal quantity of input images is dependent on the images taken.

To solve this problem, I propose to take inspiration from the linear algebra and machine learning domains to PS. From the linear algebra domain, Golub \emph{et al.}~\cite{Golub1977} studied the rank degeneracy of matrices in the least squares problem. From the machine learning domain, a whole area of research called ``feature selection'' is based on a similar idea. Both of them have a similar goal: to select some columns of a matrix while optimizing some property, such as the condition number. In order to do so, the common way to perform this is by matrix factorization. I plan on reviewing these two fields of studies to gain a global understanding of the problem. This will allow me to use the uncertainty as measure instead of the condition number, and to modify an existing algorithm to solve this problem.

Furthermore, once the optimal input images are selected, the conditioning of the problem may still be suboptimal. In that case, is there some processing that could be done on the images that could enhance the results? The bad conditioning can come from great variances in the lighting intensity. A common example is when the sun is shining at noon and a cloud occludes the sun in the afternoon. They both define a probably different MLV for a given normal. The second MLV can potentially be a very interesting vector spanning a section of the space yet unseen, but the intensity of the first one drowns it. This means that the first vector will have a norm significantly higher than the second vector. When taken in the least squares sense, the first vector will have more importance when solving than the second vector. But in reality, we don't what this to happen; both are valid pixel intensities gathered from the surface, explaining the first vector should be as important as explaining the second one.

This vector importance issue can be fixed by scaling the data by an arbitrary factor. By scaling both the input images and the computed MLVs to be of the same magnitude, the input images should be treated equally. Keeping in mind the impact of sensor noise, it is better to scale down the bright images than scaling up the dark images. The latter would boost the sensor noise, giving inaccurate data, and thus a high reconstruction uncertainty.


%\subsection{Shadow detection}
%3DV15 article with cool SD


\subsection{Algorithmic improvements: initialization refinement and merging with other techniques}
\label{subsec:calibrated}

% MRF, reference Jung's, tell our differences.
When performing a calibrated PS algorithm on environment maps, a chicken-and-egg problem arise. The part of the environment map visible by a surface patch is defined by its normal, but the normal is what we want to find using the lighting information. To solve this problem, we can sample normals uniformly over the sphere and compute their MLVs (see sec.~\ref{iccp-computingstability}. Then, we can find how well a pixel intensity in the image matches these MLVs.

This scheme may work pretty well, but lacks a fundamental element: spatial coherence, that is, how objects are usually smooth over their surface. It is safe to assume that if a given surface patch $p$ has a normal $n_p$, its neighbor $p+1$ has a similar normal: $ n_p = n_{p+1} + \epsilon $ , where $\epsilon$ is a small angle. As a way to enforce this, I propose to insert a new step to initialize the normals. To coarsely initialize the surface normals, the problem can be formalized as a Markov Random Field (MRF) by doing the hypothesis that a pixel normal is in some way related to its neighbors. An MRF can be defined as
\begin{equation}
    \arg \min_{x_{1\ldots N}} \sum_{p=1}^N \Phi (x_p) + \sum_{p=1}^N \sum_{q=n}^N \Psi(x_p, x_q)
\end{equation}
for a set of normals to recover $x$ with $N$ pixels. $\Phi$ is called the unary term, because it defines the cost of affecting a given normal $x_p$ to the pixel $p$, while $\Psi$ is called the pairwise term as it defines the cost of affecting normal $x_p$ considering its neighbor's normal $x_q$.

The unary term represents how well a normal explains the pixel intensity throughout the sequence of environment maps. This is done by computing the PS equation with the candidate normal and to compute its residual error. The residual error is the sum of all errors when predicting the pixel intensity from the MLVs over the image sequence. The higher this residual error is, the less the candidate normal explains well the pixel.

The pairwise term defines the cost of a normal given its neighbors. A simple smoothness factor could be used, for example based on the angle between two evaluated normals. An interesting remark is made by~\cite{jung-cvpr-15}. Instead of the usual smoothness prior, one could base this pairwise terms on the likeness between pixels. They argue that pixels behaving similarly through the image sequence should also have similar surface normals. Technically, they base their similarity measure on the Pearson correlation coefficient. This promising idea is worth investigating.

The idea of using an MRF is to try all the sampled normals of the sphere on all the pixels. This will give one residual error per candidate normal. All these residual errors compose the unary term of the MRF. The chosen pairwise weights explained earlier link neighboring pixels. This gives a MRF formulation that can be optimized using a standard optimization technique such as graph cuts~\cite{Boykov2001a,Kolmogorov2004a,Boykov2004,Bagon2006}.

While this MRF formulation was used recently to perform PS reconstruction~\cite{jung-cvpr-15}, it has been used only used as a final smoothing step to fix zones in shadow during most of the sequence. I believe this technique can be employed to initialize the PS procedure, followed by the standard fine-grained normal optimization, standard to PS algorithms.


% To do that, we'll need to merge some ideas from other techniques because sometimes, some pixels cannot be recovered. Silhouettes, Data-Driven priors.
The ideas previously conveyed will improve the reconstruction performance of outdoors PS and bring a better understanding of the impact of natural illumination on reconstruction algorithms based on photometric cues. But even if we improve a PS algorithm by a large margin, the analysis in sec.~\ref{iccp15} and~\ref{3dv15} showed that sometimes, even with the best possible surface with the lowest sensor noise, there is not enough photometric cues to recover certain normals or surfaces over the sphere.

To fix this issue, merging another technique with PS is the only choice. When data was captured on an unsuitable day, it will then be possible to still perform a 3D reconstruction. Previous work have shown that merging PS with Multi-View Stereo (MVS)~\cite{Beljan2012,Zhou2013,ackermann-3dv-14,HernandezEsteban2008,inose-tcva-13,shi-3dv-14} and even Structure from Motion (SfM)~\cite{zhang-iccv-03,lim-iccv-05} yield promising results. The difference between both technique is the calibration between the cameras. In Multi-View Stereo, the calibration of the cameras is known and the extrinsics between them is also known, while in Structure from Motion, the cameras are uncalibrated, which gives a projective ambiguity called the bas-relief ambiguity.

The idea I propose in the cases of unsolvable reconstruction using pure PS is to take some photos from another point of view, then performing a standard SfM algorithm to first get a coarse estimate of the scene. This will allow to fill the gaps with the dense result of PS that can constrain well on at least an axis in the worst case. For the other axis, the normal found by SfM can be propagated on neighboring vertices in conjunction with some smoothing regularization to obtain a final result.

The improvements described in this section are all planned to be part of a single PS algorithm, which is the next contribution I plan to realize. This algorithm still needs the full environment map that lit the object to perform a reconstruction, though. But the progress made with it lay the path for my next proposed contribution, an uncalibrated PS algorithm.

\section{Uncalibrated outdoor reconstruction}
\label{sec:uncalib}

% reconstruction, don't need lighting condition is a plus
All the proposed work up until now supposes we know the full environment map that lit the scene we observe. After the development of the fully calibrated algorithm described, I plan to overcome the need for capturing the sky. Getting rid of the lighting capture is a big plus, allowing the technique to be more widely applicable. A single camera would be required, simplifying the capture apparatus by a great deal.

Uncalibrated PS reconstruction has an issue that its calibrated counterpart doesn't have: the Generalized Bas-Relief (GBR) ambiguity. Because the light intensity and direction as well as the surface albedo is not known beforehand, the reconstruction of the surface height is up to an unknown factor, impossible to determine without prior knowledge of the surface or constrains external to photometric cues.

%~\cite{basri-ijcv-2007}
Authors over the years have proposed two main ideas to surmount the GBR ambiguity. Doing it manually (i.e., forcing a depth to an arbitrary value) seems to be a quite common way to deal with this issue. Most researchers, though, proposed a wide amount of priors and optimization schemes on the surface albedo~\cite{tan-cvpr-07,alldrin-cvpr-08,abrams-eccv-12,queau-jmiv-14} to solve this problem.

% Data-driven approach, what can the DB tell us on the image.
%Bas-Relief Amb.
In order to make PS work uncalibrated, I plan to use a data-driven approach. By taking advantage of the sky database we already have, we can lead an algorithm to learn what a physically plausible sky looks like, constraining greatly the problem. The quantity of real world skies that can explain well enough the illumination on a scene is quite small, if we take the similar skies as one. \todo{Not clear how this is going to help with the GBR}

I suspect this lighting constraint to be enough to alleviate the GBR ambiguity issue. In the case this constraint is not enough, I plan to use an optimization method on the material albedo as proposed by the authors previously mentioned.

Overall, the GBR ambiguity is a minor issue with many techniques to solve it available in the literature. What is left to do, though, is an uncalibrated outdoor single day PS algorithm that brings performances similar to the laboratory conditions. The current state-of-the-art technique~\cite{jung-cvpr-15} obtains between 30\% and 40\% of the recovered normals under 30\degree of error, which is far away from the impressive precision obtained using PS in laboratory conditions.

In the following, I propose a succession of steps in order to take the calibrated algorithm proposed earlier and push it to a fully uncalibrated one.


%\subsection{Reconstructing the ground}
% First, reconstruction of the ground only.
%A full environment map is hard to obtain. This means that a full sphere of irradiance must be captured. Usually, sky capture apparatus is composed of a wide angle lens pointing toward up (zenith), capturing only at best a small portion of the horizon.
%Building a full environment map is done by adding the ground, which requires the stitching of two captures.

%The first enhancement to the system I propose is to get rid of the ground by estimating it. As of now, in our previous contributions we estimate the ground to be a single constant, repeated over the bottom (nadir) hemisphere. This is a good enough approximation on objects that have been laid carefully on a uniform piece of cloth, but is likely to break on real building-scale structures.

%To fix this issue, it is possible to segment the 


\subsection{Using only the sun position}
\label{subsec:knowingsunposition}

\todo{reorganise: 1) Knowing the sun posibition w.r.t. camera. 2) knowing sun position in the sky, but not the camera orientation.}
% Then, estimation of the sky using the GPS coordinates and date & time. Then, without date and time.
In order to move toward an uncalibrated algorithm, the next step will be to remove the environment map altogether from the inputs of the method. Instead, I propose to work only with the GPS coordinates alongside with the date and time of image acquisition, available in the EXIF data of pictures taken.

With the GPS coordinates and the date and time, it is possible to know precisely where the sun was in the sky. With this information, it is possible to determine whether the sun is occluded by clouds or not, depending on the intensity of the object image. If it is not occluded, the MLV of pixels not in shadow are constrained to be at the computed elevation of the sun.

By using previous work on finding the low frequency illumination~\cite{basri-ijcv-2007} or by using matrix factorization techniques~\cite{shi-cvpr-10}, it is possible to know the maximum light direction. If we have at least one image where the sun is shining onto the object, is it then possible to assign this maximum light direction to the sun, resolving the rotation ambiguity.

The sky could then be estimated by a two component parametric model of the sky, as in~\cite{jung-cvpr-15}. I propose instead to use a more powerful data-driven sky model such as~\cite{lalonde-3dv-14}. take advantage of our sky database in order to either find or synthesize a new sky explaining the observed scene. This approach has the advantage of producing real-looking physically-based environment maps.

%Synthesizing new skies can be a complex task. When talking about synthesizing, the meaning is basically to find an environment map that fits globally the required specifications and, at need, rotate it so the energy comes from the desired directions. More advanced synthesizing techniques could be interesting to work with, for example generating a new environment map using texture synthesis, and will be tested if the need arise.

As an intermediate step, it is possible to remove knowledge of the camera orientation as an input of the algorithm. Previously, we knew exactly the azimuth and elevation of the sun in the sky. Now, these values are constrained to an arc in the sky. I still assume the camera and object orientation are unknown. Now, using two images where the sun is not occluded, it is possible to solve this problem. Still using the maximum light direction technique, once two light directions are found, it is possible to find the camera azimuth (or object rotation). Using more images than two will provide an overconstrained system of equation, solvable using a standard least squares approach. As a result, the capture time can also be estimated.


\subsection{Fully uncalibrated}
\label{subsec:fullyuncalibrated}
% Lastly fully uncalibrated
The last step is to provide a fully uncalibrated PS algorithm. Building on the previously proposed technique, it is possible to remove the GPS coordinates and the date from the technique. It is expected that at least some captures over the sequence will be sunlit, yielding a high overall image intensity and giving a clear principal light direction using the maximum light direction scheme explained earlier. It is thus possible to find the sun arc in the sky, enabling self-calibration. By fitting these principal light directions to environment maps available in the database, it will be possible not only to obtain the scene structure but to estimate the lighting that made the photo.

This proposed technique will be a major contribution that iterates over the previous state of the art by adding a richer data-driven sky illumination model at its core.


\section{Night and Day Photometric Stereo}
\label{sec:nightandday}

% Up until now, we only considered the atmosphere during the day for reconstruction. The main problem, coplanar.
All the previous proposed work only considered daytime lighting conditions. The main problem, which is the coplanarity of the sun trajectory over the course of a day, can be solved by richer and more complex sky models as explained in the previous sections. Another interesting avenue is to revert to a simpler model: the point light source, which yields an underconstrained problem as explained in sec.~\ref{sec:ps_ori}.

With this analysis, though, it is necessary to bring something else into play in order to better constrain the problem. I propose to use the sunlight during the day, and the moonlight at night, to perform PS. It is hard to find something capable of competing with the sun in terms intensity in the sky through a day, but the moon can be up to par in terms of quantity of information needed to constrain the problem correctly.
%Another avenue is to manually modify the lighting, either by distorting the sun light or adding new light sources.


%\subsection{Day \& Night PS}
When seen during the day, the moon doesn't bring much lighting to a scene. At night, it can be strong enough to cast clearly visible and sharp shadows on the ground. The main interesting thing about the moon is that it presents, most of the time, lighting from a plane different than the sun. I plan to simplify the illumination model by letting the capture continue overnight.

Over the course of 24 hours, both sun and moon trajectories are mostly coplanar. Examples of such trajectories are shown in fig.~\ref{fig:DN-sunmoon-trajectories}. Both of them are parallel, which is not surprising because the earth, the sun and the moon are roughly on the same plane in the solar system.

\begin{figure}
\centering
\begin{tabular}{cc}
\includegraphics[height=0.43\linewidth]{DayNight/20150225_lat47.png} &
\includegraphics[height=0.43\linewidth]{DayNight/20150225_lat75.png} \\
(a) & (b)
\end{tabular}
\caption{Example of the trajectories of the sun and the moon for 2015-02-25 at latitude (a) 47 and (b) 75. Two things are interesting to note: First, the moon trajectory suddenly stops because the sun rises at this moment. The moon is then not considered as a dominant light source anymore. Secondly, as the latitude gets higher, the trajectories becomes more and more parallel to the horizon.}
\label{fig:DN-sunmoon-trajectories}
\end{figure}

We can use the same measure as in sec.~\ref{3dv15} to analyze the PS reconstruction quality, that is, the maximum gain. This maximum gain is the factor that multiplies, or amplifies, the sensor noise. The higher it is, the more uncertain (or wrong) the reconstruction will be. So the lower is the maximum gain, the better will be the reconstruction performance.

Such an analysis for year 2015 is found in fig.~\ref{fig:DN-maximum_gain} for the location of Laval University. The two major peaks around days 81 and 264 are due to the spring and fall equinoxes, respectively. The sun rises exactly east and sets exactly west at both equinoxes, making the lighting vectors plane all coplanar. As predicted by \cite{shen-pg-14}, all lighting vectors lie on the same plane as the center of the sphere at these moments, whereas at the solstice, the lighting vectors are farther from a single plane.

Another interesting thing to notice is the small sinusoid with a 13.66 days period present over the whole year. This is linked to the sidereal period of the moon, namely the time it takes to make a full orbit. This small sinusoid takes exactly half the sidereal period of the moon, 27.32 days, as shown in fig.~\ref{fig:DN-maximum_gain}. The moments when the gain is high coincides with the moon equinoxes, when the moon rises exactly east and sets west, happening twice per period. This means that all the MLVs throughout the night are lying on the same plane, conditioning the problem badly.

It is interesting to see that adding the moon to the capture increases the reconstruction performance most of the time, aside for two cases: 1) when it is a moon equinox (moon raising east and setting west) and 2) when the night is moonless. Such cases are shown in fig.~\ref{fig:badcases} and explains the highest peaks of fig.~\ref{fig:DN-maximum_gain}. The best days of using the moon as a light source are visible during the earth equinoxes. These times of the year can still hold a good expected reconstruction performance when adding the moon, provided it is visible during the night and far from the moon equinox.

By comparing figures~\ref{fig:DN-maximum_gain_sunmoon} and~\ref{fig:DN-maximum_gain}, it is interesting to note that when both are not at their respective equinoxes, the expected performance is better than taking either as sole light source. In the best case, using both the sun and the moon can boost the maximum gain to be 12\% better on the solstices (maximum gain of 0.53 instead of 0.60). What is interesting is that it actually allows a reconstruction only 26\% worse than the best possible case in the case of an earth equinox (maximum gain of 0.67 when the moon is far from its equinox), which is way better than the completely unconstrained rank-2 problem when using only the sun.

\begin{figure}
\centering
\includegraphics[height=8cm]{DayNight/maximum_gain_withsun.pdf}
\caption{Maximum gain for PS reconstruction (lower is better) assuming only the sun as light source (solid blue line) and with the sun and moon over 24 hours (color-coded ine) for the year 2015. The color-coded moon illumination represent the phase of the moon, full moon being 1 and new moon being 0. This analysis is done for the Laval University campus location (Latitude and Longitude = 46.779077, -71.275778 ).}
\label{fig:DN-maximum_gain}
\end{figure}

\begin{figure}
\centering
\includegraphics[height=8cm]{DayNight/maximum_gain_sunmoon.pdf}
\caption{Maximum gain for PS reconstruction (lower is better) assuming only the sun or the moon as light source. This analysis is done for the Laval University campus location (Latitude and Longitude = 46.779077, -71.275778 ).}
\label{fig:DN-maximum_gain_sunmoon}
\end{figure}

\begin{figure}
\centering
\begin{tabular}{cc}
\includegraphics[height=0.49\linewidth]{DayNight/20150307_lat47.png} &
\includegraphics[height=0.49\linewidth]{DayNight/20150320_lat47.png} \\
(a) & (b)
\end{tabular}
\caption{Trajectory of the sun and the moon for 2015-03-07 at latitude 47\degree. Both those days correspond to (a) the peak right before and (b) at earth's spring equinox in fig.~\ref{fig:DN-maximum_gain}. Note how the problem is badly conditioned in both cases, where both trajectories are either superimposed (a) or the night is mostly moonless (b).}
\label{fig:badcases}
\end{figure}

Another notion worth of mention is the moon illumination, related to its synodic period. In contrast to the sidereal period, the synodic period, also called the lunar cycle for the moon, is the time taken before it reappears at the same point in the sky, which is 29.5 days. Over the course of its cycle, the moon will have an illumination of
\begin{equation}
k = \frac{1 + \cos(\phi)}{2}  \quad,
\end{equation}
where $\phi$ is the lunar phase in radians~\cite{Meeus1991}. While this idea will obviously not work when the illumination is null, the rest of the time, the reconstruction performance will be highly dependent upon the light pollution at the capture location. This light pollution can be approximated by an ambient illumination term, which is analyzed by~\cite{Angelopoulou2013}.

Statues, historical bridges and architectural monuments are often lit using spotlights during the night. During a new moon, such spotlights could be used as substitutes to the moon in this analysis, providing another way to constrain correctly the PS problem.

%The main problem with this idea, as can be seen from the unusual astronomical arguments made in this section, is the knowledge transfer between the astronomical domain and computer vision. We can assume that most of the people aware of PS have little to no knowledge of the side effects of the cycle of the moon and its trajectory in the sky. By investigating this field of study and summarizing the pertinent informations, I am certain that performing day \& night PS will bring a nice contribution to the community. Furthermore, knowledge gained in low lighting conditions such as the night will likely be profitable on some daylight illumination, such as cloudy days.


%\subsection{Artificial alterations}

%The sun, when taken as a point light source, may have a coplanar trajectory over the course of a day, but nothing prevents the addition or modification of this lighting.

% Diffusers

% City lights


%\subsection{Other techniques}



%\section{Augmentation avec d'autres techniques}

% from ICCP
%While these techniques can lead to well-defined solutions, they are not always practical in many scenarios with strict temporal or geographical constraints. A second strategy has therefore been to combine PS with other techniques such as multi-view stereo~\cite{inose-tcva-13,shi-3dv-14}, or use reference objects as in \cite{johnson-cvpr-11} or example-based PS~\cite{hertzmann-pami-05,ackermann-3dv-14}. But can we accurately reconstruct surface geometry simply based on the photometric cue in an outdoor setting, without overly restrictive temporal and geographical constraints?

%Dcrire l'amlioration que pourrait apporter la PS utilise conjointement avec du SFM et la stro multivues standard.